{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tz915KMfGVC4"},"outputs":[],"source":["!pip install -q esprima"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZ2ymXUqWOeQ"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, precision_recall_curve, classification_report, confusion_matrix\n","from keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, Conv1D, GlobalMaxPooling1D\n","from keras.callbacks import ModelCheckpoint, EarlyStopping,Callback,CSVLogger\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.feature_extraction.text import CountVectorizer\n","from keras.layers import Activation, Dense, Reshape, Input\n","from sklearn.model_selection import train_test_split,KFold\n","from sklearn.preprocessing import StandardScaler\n","from keras.models import Sequential, load_model\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import Model , clone_model\n","from keras.losses import BinaryCrossentropy\n","from keras.backend import clear_session\n","from keras.initializers import Constant\n","from nltk.tokenize import word_tokenize\n","from threading import current_thread\n","from keras.utils import plot_model\n","from sklearn.utils import shuffle\n","from keras.optimizers import Adam\n","from keras.utils import Sequence\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from keras import backend as K\n","from functools import partial\n","from numpy import asarray\n","import concurrent.futures\n","from keras import layers\n","import matplotlib as mpl\n","from keras import Model\n","from numpy import save\n","from numpy import load\n","import seaborn as sns\n","import esprima as esp\n","from time import time\n","import pandas as pd\n","import numpy as np\n","import tempfile\n","import logging\n","import urllib\n","import random\n","import pickle\n","import pydot\n","import torch\n","import math\n","import html\n","import nltk\n","import os\n","import re\n","import gc\n","np.random.seed(123)\n","plt.style.use('ggplot')\n","# mpl.rcParams['figure.figsize'] = (12, 10)\n","colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58408,"status":"ok","timestamp":1671090181786,"user":{"displayName":"Fatrmah rac","userId":"03011577188967940535"},"user_tz":480},"id":"SxgWX6jZGVC7","outputId":"c5136b85-17f0-4f88-a8ac-e265cdae12a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\",force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ku-SWpUoGVC7"},"outputs":[],"source":["par_path = \"/content/drive/MyDrive/Colab Notebooks/Muawiya/Js_Contana/On Payload File/Hiden\"\n","model_info_path = \"/content/drive/MyDrive/Colab Notebooks/Muawiya/Js_Contana/On Payload File/Results/Model info\"\n","results_path = \"/content/drive/MyDrive/Colab Notebooks/Muawiya/Js_Contana/On Payload File/Results\"\n","data_path = \"/content/drive/MyDrive/Colab Notebooks/Muawiya/Js_Contana/On Payload File/DataSets\"\n","codes_path = \"/content/drive/MyDrive/Colab Notebooks/Muawiya/Js_Contana/On Payload File/Codes\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKcTOwH2GVC8"},"outputs":[],"source":["class GarbageCollectorCallback(Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TArltu6bxHpx"},"outputs":[],"source":["def print_result(y_pred, y_test , title ,color):\n","    if len(y_test) < len(y_pred):\n","        y_pred = y_pred[: len(y_test)]\n","    elif len(y_test) > len(y_pred):\n","        y_test = y_test[: len(y_pred)]\n","    accuracy = accuracy_score(y_pred, y_test)\n","    classification_rep = classification_report(y_test, y_pred)\n","    con_matrix = confusion_matrix(y_test, y_pred)\n","    if len(con_matrix)==1:\n","        if len(con_matrix[0])==1:\n","            con_matrix = list(con_matrix)\n","            con_matrix[0] = list(con_matrix[0])\n","            con_matrix[0].append(0)\n","            con_matrix = np.array([con_matrix[0],[0,0]])\n","    plt.figure(figsize=(25,15))\n","    plt.subplot(2,1,1)\n","    group_names =['Not one']\n","\n","    if y_test is np.ndarray:\n","      T1 = np.count_nonzero(y_test == 0) if np.count_nonzero(y_test == 0) !=0 else 1\n","      T2 = np.count_nonzero(y_test == 1)  if np.count_nonzero(y_test == 1) !=0 else 1\n","    else:\n","      T1 = y_test.count(0) if y_test.count(0)!=0 else 1\n","      T2 = y_test.count(1) if y_test.count(1)!=0 else 1\n","    group_counts = con_matrix\n","    group_percentages = np.round(con_matrix / np.array([[T1 , T1],[T2 , T2]]),3)\n","    t = 0.5\n","    plt.text(x=0.0 , y = 2.6, s =  \"confusion_matrix :\",fontsize=15)\n","    plt.text(x=0.0 , y = 3.3-t, s =  'Not one ',fontsize=15)\n","    plt.text(x=0.0 , y = 3.4-t, s =  'One ',fontsize=15)\n","    plt.text(x=0.3 , y = 3.2-t, s =  'Not one ',fontsize=15)\n","    plt.text(x=0.6 , y = 3.2-t, s =  'One',fontsize=15)\n","    plt.text(x=0.3 , y = 3.3-t, s =  \"{}\".format(con_matrix[0][0]),fontsize=15)\n","    plt.text(x=0.6 , y = 3.3-t, s =  \"{}\".format(con_matrix[0][1]),fontsize=15)\n","    plt.text(x=0.3 , y = 3.4-t, s =  \"{}\".format(con_matrix[1][0]),fontsize=15)\n","    plt.text(x=0.6 , y = 3.4-t, s =  \"{}\".format(con_matrix[1][1]),fontsize=15)\n","    sns.heatmap(group_percentages,xticklabels= ['Not one' , 'One'], yticklabels=['Not one' , 'One']  , annot=True, annot_kws={\"size\": 16},fmt='g', cmap=color)\n","    plt.title(title ,  fontsize = 18)\n","    plt.text(x =0,y = 2.3 , s = \"accuracy  score on test : {}\".format(accuracy),fontsize=15)\n","    plt.text(x=0 , y = 3.9, s =  \"classification_report :\\n {}\".format(classification_rep),fontsize=15)\n","    plt.xlabel(\"Predicted value\")\n","    plt.ylabel(\"Real value\")\n","    plt.savefig(\"{} .png\".format(os.path.join(os.path.join(results_path,\"Image\"),title)))\n","    # plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFL3dqr-HwsW"},"outputs":[],"source":["def _replaceitem(x):\n","    if type(x) is list:\n","        if x[0]<0.5:\n","            return 0.0\n","    else:\n","        x = float(x)\n","        if x<0.5:\n","            return 0.0\n","    return 1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hg_OhoRtGVC-"},"outputs":[],"source":["def save_object(obj, filename,path):\n","    \"\"\"\n","    _ INPUT (obj) THE OBJECT WE NEED SAVW IT (filename) THE NAME OF OBJECT\n","    \"\"\"\n","    filename = os.path.join(path,filename)\n","    with open(filename+\".pkl\", 'wb') as outp:\n","        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n","    outp.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W8sQUojTGVC_"},"outputs":[],"source":["def load_object(filename,path):\n","    \"\"\"\n","    _ INPUT THE NAME OF OBJECT WE NEED LOAD IT\n","    \"\"\"\n","    filename = os.path.join(path,filename)\n","    with open(filename+\".pkl\", 'rb') as outp:\n","        loaded_object = pickle.load(outp)\n","    outp.close()\n","    return loaded_object"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XkjCtzTnGVC_"},"outputs":[],"source":["def plot_history(history,title):\n","    acc = history.history['accuracy']\n","    val_acc = history.history['val_accuracy']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    x = range(1, len(acc) + 1)\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(x, acc, 'b', label='Training acc')\n","    plt.plot(x, val_acc, 'r', label='Validation acc')\n","    plt.title('Training and validation accuracy')\n","    plt.legend()\n","    plt.subplot(1, 2, 2)\n","    plt.plot(x, loss, 'b', label='Training loss')\n","    plt.plot(x, val_loss, 'r', label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.savefig(\"{} .png\".format(os.path.join(os.path.join(results_path,\"Image\"),\"history \"+title+v)))\n","    plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8rL16qkxWhW"},"outputs":[],"source":["def Pad_Sequences(scripts,fit = True,creat=True):\n","    if fit:# and creat:\n","        tokenizer = Tokenizer(num_words=num_words)\n","        tokenizer.fit_on_texts(scripts)\n","        save_object(tokenizer, \"Pad_Sequences_tokenizer\",results_path)\n","    # elif fit and not creat:\n","    #     tokenizer = load_object(\"Pad_Sequences_tokenizer\",results_path)\n","    #     tokenizer.fit_on_texts(scripts)\n","    #     save_object(tokenizer, \"Pad_Sequences_tokenizer\",results_path)\n","    else :\n","        tokenizer = load_object(\"Pad_Sequences_tokenizer\",results_path)\n","    X = tokenizer.texts_to_sequences(scripts)\n","    X = pad_sequences(X, padding='post', maxlen=maxlen)\n","    return X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6P5fdmKUSg4"},"outputs":[],"source":["metric_functions = {'accuracy': accuracy_score, 'precision': precision_score,\n","                        'recall': recall_score, 'f1_score': f1_score}\n","def cummulative_metrics(y_true, y_pred, step_size, metric):\n","    metric_score = []\n","    for i in range(step_size, len(y_true) - step_size, step_size):\n","        metric_score.append(metric_functions[metric](y_true[:i], y_pred[:i]))\n","    return metric_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4es2BsAdUT97"},"outputs":[],"source":["def time_series_plot(df,  metric, step_size=40, title=\" \", method_names=None, drift_data_path=None):\n","    fig = plt.figure(figsize=(20, 12))\n","    colors = ['r', 'g', 'b', 'm', 'c']\n","    y_true, y_pred = None,None\n","    for d in range(df.shape[0]):\n","        if y_true is None:\n","\n","            y_true, y_pred = df['y_test'][d] , list(df['y_predict'][d])\n","            # y_true = list(map(_replaceitem, y_true))\n","            # y_pred = list(map(_replaceitem, y_pred))\n","        else:\n","            y_true = y_true + df['y_test'][d]\n","            y_pred = y_pred + list(df['y_predict'][d])\n","            # y_true = list(map(_replaceitem, y_true))\n","            # y_pred = list(map(_replaceitem, y_pred))\n","    metric_score = cummulative_metrics(y_true, y_pred, step_size, metric)\n","    X = np.linspace(0, 16000, len(metric_score))\n","    plt.plot(X, metric_score, label='{}'.format(title))\n","    plt.xlabel(\"Time Series\")\n","    plt.ylabel(\"{}\".format(metric))\n","    plt.title(title)\n","    plt.legend()\n","    plt.savefig(os.path.join(os.path.join(results_path,\"Image\"),\"{}_{}.png\".format(title, metric)))\n","    # plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0wZJWnTGVDB"},"outputs":[],"source":["def create_model(shape=None,without_first_layer=False):\n","    \"\"\"\n","    _INPUT (without_first_layer) IF WE WANT TO CREAT MODEL WITHOUT FIRST LSTM LAYER (shape) IF THE PREVIOS PAREMETER IS TRUE WE NEED DIFINE THE SHAPE FOR INPUT LAYER\n","    _OUTPUT THE MODEL\n","    \"\"\"\n","    clear_session()\n","    model = Sequential()\n","    if without_first_layer:\n","        name_model = 'without_biLSTM_layers_weights.best.hdf5'\n","        model.add(layers.InputLayer(input_shape=shape))\n","        # model.add(Bidirectional(LSTM(units=50, input_shape=(None, 50), return_sequences=True)))\n","    else:\n","        name_model = 'with_biLSTM_layers_weights.best.hdf5'\n","        model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n","        model.add(Bidirectional(LSTM(units=50, input_shape=(None, 50), return_sequences=True)))\n","        model.add(Bidirectional(LSTM(units=50, input_shape=(None, 50), return_sequences=True)))\n","    # TextCNN with 4 conv layers\n","    model.add(Conv1D(128, 7, activation='tanh', input_shape=(None, 32)))\n","    model.add(Conv1D(128, 15, activation='tanh'))\n","    model.add(Conv1D(128, 25, activation='tanh'))\n","    model.add(Conv1D(128, 35, activation='tanh'))\n","    model.add(GlobalMaxPooling1D())\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n","    model.load_weights(os.path.join(model_info_path,name_model))\n","    plot_model(model, to_file=os.path.join(model_info_path,name_model[:-10]+'.png'), show_shapes=True, show_layer_names=True)\n","    # model.summary()\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l4KFkG8DGVDC"},"outputs":[],"source":["def creat_metrics():\n","    model_comparison_table = {}\n","    model_comparison_table['chunk_number'] = []\n","    model_comparison_table['y_test'] = []\n","    model_comparison_table['y_predict'] = []\n","    return model_comparison_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FT0ODMi7GVDC"},"outputs":[],"source":["def evaluate_result(model_comparison_table,y_predict,y_test,history,title,chunk_number,mytype):\n","    c = 'Blues'\n","    if mytype == 1:\n","      c = 'Accent'\n","    y_predict[y_predict>0.5] = 1\n","    y_test = list(map(_replaceitem, y_test))\n","    y_predict = list(map(_replaceitem, y_predict))\n","    model_comparison_table['y_test'].append(y_test)\n","    model_comparison_table['y_predict'].append(y_predict)\n","    model_comparison_table['chunk_number'].append(title+\"chunk_number_\"+str(chunk_number))\n","    print_result(y_predict, y_test , title+\"chunk_number_\"+str(chunk_number),color=c)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqGeTzVaGVDD"},"outputs":[],"source":["def chunking_data(Data,test_size=0.20):\n","    \"\"\"\n","    _INPUT (Data) CHUNK OF DATASET (test_size=0.20) SIZE OF SPLITNG\n","    _OUTPUT (X_train , y_train , X_test , y_test)\n","    \"\"\"\n","    train_data, test_data = train_test_split(Data, test_size=0.2,random_state=42,shuffle=False)\n","    X_test = test_data[test_data.columns[-2]]\n","    y_test = test_data[test_data.columns[-1]]\n","    X_train = train_data[train_data.columns[-2]]\n","    y_train = train_data[train_data.columns[-1]]\n","    return X_train , y_train , X_test , y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oevNVDbOGVDE"},"outputs":[],"source":["model_result_table_1 = creat_metrics()\n","model_result_table_2 = creat_metrics()\n","\n","\n","vocab_size = 500000\n","num_words = 400000\n","maxlen = 2048\n","embedding_dim = 512\n","\n","model_is_not_created = True\n","\n","filepath_1 = os.path.join(model_info_path,'with_biLSTM_layers_weights.best.hdf5')\n","history_logger_1 = CSVLogger(os.path.join(model_info_path,'history1.csv'), separator=\",\", append=True)\n","checkpoint_1 = ModelCheckpoint(filepath_1, monitor='loss', verbose=0, save_best_only=True, mode='min')\n","es_1 = EarlyStopping(monitor='loss', patience=5)\n","callbacks_list_1 = [GarbageCollectorCallback(),checkpoint_1, es_1,history_logger_1]\n","\n","filepath_2 = os.path.join(model_info_path,'without_biLSTM_layers_weights.best.hdf5')\n","history_logger_2 = CSVLogger(os.path.join(model_info_path,'history2.csv'), separator=\",\", append=True)\n","checkpoint_2 = ModelCheckpoint(filepath_2, monitor='loss', verbose=0, save_best_only=True, mode='min')\n","es_2 = EarlyStopping(monitor='loss', patience=5)\n","callbacks_list_2 = [GarbageCollectorCallback(),checkpoint_2, es_2,history_logger_2]\n","\n","model1 = create_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7YsxXgHCqvR"},"outputs":[],"source":["for i,df_iterator in tqdm(enumerate(pd.read_csv(os.path.join(data_path,\"XSS_dataset_payload.csv\"),chunksize=2000))):\n","  df_iterator = df_iterator.mask(df_iterator.eq(np.nan)).dropna()\n","  X_train , y_train , X_test , y_test  = chunking_data(df_iterator)\n","  X_train = Pad_Sequences(X_train,fit = True)\n","  X_test = Pad_Sequences(X_test,fit=False) # ,creat=False\n","  Embeding_word_from_first_lstm = K.function([model1.input], [model1.layers[1].output])\n","  Embeding_word_from_second_lstm = K.function([model1.input], [model1.layers[2].output])\n","  history1 = model1.fit(X_train,y_train,epochs=10,batch_size=100,validation_split=0.20,callbacks=callbacks_list_1, verbose=0)\n","  evaluate_result(model_result_table_1,model1.predict(X_test),y_test,history1,\"model_with_biLSTM_layres_\",i,0)\n","  Intermediate_word_embeding_from_first_lstm = Embeding_word_from_first_lstm([X_train, 1])[0]\n","  Intermediate_word_embeding_from_second_lstm = Embeding_word_from_second_lstm([X_train, 1])[0]\n","\n","  temp = []\n","  for k in range(X_train.shape[0]):\n","      temp.append(np.reshape(X_train[k], (X_train[k].shape[0], 1)))\n","  Intermediate_word_embeding_from_static_word_embeding = np.array(temp)\n","  Dynamic_word_embeding = 0.8*((0.45*Intermediate_word_embeding_from_second_lstm)+(0.35*Intermediate_word_embeding_from_first_lstm)+(0.20*Intermediate_word_embeding_from_static_word_embeding))\n","\n","  save_object(Dynamic_word_embeding, \"X_train_hiden_for_chunk_\"+str(i),par_path)\n","  save_object(y_train, \"y_train_hiden_for_chunk_\"+str(i),par_path)\n","  sub_model_1 = Model(inputs=model1.input,outputs=model1.layers[1].output)\n","  sub_model_2 = Model(inputs=model1.input,outputs=model1.layers[2].output)\n","  X_test_1 = sub_model_1.predict(X_test)\n","  X_test_2 = sub_model_2.predict(X_test)\n","\n","  temp2 = []\n","  for j in range(X_test.shape[0]):\n","    temp2.append(np.reshape(X_test[j], (X_test[j].shape[0], 1)))\n","  Intermediate_word_embeding_from_static_word_embeding2 = np.array(temp2)\n","  X_test = 0.8*((0.45*X_test_2)+(0.35*X_test_1)+(0.20*Intermediate_word_embeding_from_static_word_embeding2))\n","  if model_is_not_created:\n","    model2 = create_model(Dynamic_word_embeding[0].shape,without_first_layer=True)\n","    model_is_not_created = False\n","  history2 = model2.fit(Dynamic_word_embeding,y_train,epochs=10,batch_size=100,validation_split=0.20,callbacks=callbacks_list_2, verbose=0)\n","  evaluate_result(model_result_table_2,model2.predict(X_test),y_test,history2,\"model_without_biLSTM_layres_\",i,1)\n","  save_object(X_test, \"X_test_hiden_for_chunk_\"+str(i),par_path)\n","  save_object(y_test, \"y_test_hiden_for_chunk_\"+str(i),par_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V5y0yVxalJ2w"},"outputs":[],"source":["print(\"Save Result ....\")\n","save_object(model_result_table_1, \"model_result_table_1\",results_path)\n","model1.save(os.path.join(model_info_path,\"model1.h5\"))\n","save_object(model_result_table_2, \"model_result_table_2\",results_path)\n","model2.save(os.path.join(model_info_path,\"model2.h5\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VW_4MUnbGVDF"},"outputs":[],"source":["df_1 = pd.DataFrame(model_result_table_1)\n","df_1.to_csv(os.path.join(results_path,\"model_result_table_1.csv\"),index=False)\n","df_2 = pd.DataFrame(model_result_table_2)\n","df_2.to_csv(os.path.join(results_path,\"model_result_table_2.csv\"),index=False)"]},{"cell_type":"code","source":["df_1 = pd.read_csv(os.path.join(results_path,\"model_result_table_1.csv\"))\n","df_2 = pd.read_csv(os.path.join(results_path,\"model_result_table_2.csv\"))"],"metadata":{"id":"_3FAq_Ppt2Or"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SBFwP6YzGVDH"},"outputs":[],"source":["# metrics = ['accuracy','precision','recall','f1_score']\n","# for metric in metrics:\n","#     time_series_plot(df_1,  metric,title=\"Model_with_biLSTM_layers\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-YGydzypykZ"},"outputs":[],"source":["# metrics = ['accuracy','precision','recall','f1_score']\n","# for metric in metrics:\n","#     time_series_plot(df_2,  metric,title=\"Model_without_biLSTM_layers\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWisTAm4YXvp"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}