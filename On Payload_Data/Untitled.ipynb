{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Tfidf\n",
    "vectorizer = TfidfVectorizer(input=\"content\", max_features=None, tokenizer=dummy_fun, preprocessor=dummy_fun)\n",
    "vectorizer.fit(df_data[\"c\"])\n",
    "df_tfidf = vectorizer.transform(df_data[\"c\"])\n",
    "df_tfidf = df_tfidf.toarray()\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocab = np.array(vocab)\n",
    "df_tfidf = pd.DataFrame(df_tfidf,columns=vocab)\n",
    "# view rawbiblegcn_tfidf hosted with ❤ by GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### PMI between words\n",
    "window = 10 # sliding window size to calculate point-wise mutual information between words\n",
    "names = vocab\n",
    "n_i  = OrderedDict((name, 0) for name in names)\n",
    "word2index = OrderedDict( (name,index) for index,name in enumerate(names) )\n",
    "occurrences = np.zeros( (len(names),len(names)) ,dtype=np.int32)\n",
    "\n",
    "# Find the co-occurrences:\n",
    "no_windows = 0\n",
    "for l in tqdm(df_data[\"c\"], total=len(df_data[\"c\"])):\n",
    "    for i in range(len(l)-window):\n",
    "        no_windows += 1\n",
    "        d = set(l[i:(i+window)])\n",
    "\n",
    "        for w in d:\n",
    "            n_i[w] += 1\n",
    "        for w1,w2 in combinations(d,2):\n",
    "            i1 = word2index[w1]\n",
    "            i2 = word2index[w2]\n",
    "\n",
    "            occurrences[i1][i2] += 1\n",
    "            occurrences[i2][i1] += 1\n",
    "\n",
    "### convert to PMI\n",
    "p_ij = pd.DataFrame(occurrences, index = names,columns=names)/no_windows\n",
    "p_i = pd.Series(n_i, index=n_i.keys())/no_windows\n",
    "\n",
    "for col in p_ij.columns:\n",
    "    p_ij[col] = p_ij[col]/p_i[col]\n",
    "for row in p_ij.index:\n",
    "    p_ij.loc[row,:] = p_ij.loc[row,:]/p_i[row]\n",
    "p_ij = p_ij + 1E-9\n",
    "for col in p_ij.columns:\n",
    "    p_ij[col] = p_ij[col].apply(lambda x: math.log(x))\n",
    "# view rawbible_gcn_pmi hosted with ❤ by GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_word_edges(p_ij):\n",
    "    word_word = []\n",
    "    cols = list(p_ij.columns); cols = [str(w) for w in cols]\n",
    "    for w1, w2 in tqdm(combinations(cols, 2), total=nCr(len(cols), 2)):\n",
    "        if (p_ij.loc[w1,w2] > 0):\n",
    "            word_word.append((w1,w2,{\"weight\":p_ij.loc[w1,w2]}))\n",
    "    return word_word\n",
    "    \n",
    "### Build graph\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(df_tfidf.index) ## document nodes\n",
    "G.add_nodes_from(vocab) ## word nodes\n",
    "### build edges between document-word pairs\n",
    "document_word = [(doc,w,{\"weight\":df_tfidf.loc[doc,w]}) for doc in df_tfidf.index for w in df_tfidf.columns]\n",
    "G.add_edges_from(document_word)\n",
    "### build edges between word-word pairs\n",
    "word_word = word_word_edges(p_ij)\n",
    "G.add_edges_from(word_word)\n",
    "# view rawbuild_graph hosted with ❤ by GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class gcn(nn.Module):\n",
    "    def __init__(self, X_size, A_hat, bias=True): # X_size = num features\n",
    "        super(gcn, self).__init__()\n",
    "        self.A_hat = torch.tensor(A_hat, requires_grad=False).float()\n",
    "        self.weight = nn.parameter.Parameter(torch.FloatTensor(X_size, 330))\n",
    "        var = 2./(self.weight.size(1)+self.weight.size(0))\n",
    "        self.weight.data.normal_(0,var)\n",
    "        self.weight2 = nn.parameter.Parameter(torch.FloatTensor(330, 130))\n",
    "        var2 = 2./(self.weight2.size(1)+self.weight2.size(0))\n",
    "        self.weight2.data.normal_(0,var2)\n",
    "        if bias:\n",
    "            self.bias = nn.parameter.Parameter(torch.FloatTensor(330))\n",
    "            self.bias.data.normal_(0,var)\n",
    "            self.bias2 = nn.parameter.Parameter(torch.FloatTensor(130))\n",
    "            self.bias2.data.normal_(0,var2)\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.fc1 = nn.Linear(130,66)\n",
    "        \n",
    "    def forward(self, X): ### 2-layer GCN architecture\n",
    "        X = torch.mm(X, self.weight)\n",
    "        if self.bias is not None:\n",
    "            X = (X + self.bias)\n",
    "        X = F.relu(torch.mm(self.A_hat, X))\n",
    "        X = torch.mm(X, self.weight2)\n",
    "        if self.bias2 is not None:\n",
    "            X = (X + self.bias2)\n",
    "        X = F.relu(torch.mm(self.A_hat, X))\n",
    "        return self.fc1(X)\n",
    "# view rawgcn_net hosted with ❤ by GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
