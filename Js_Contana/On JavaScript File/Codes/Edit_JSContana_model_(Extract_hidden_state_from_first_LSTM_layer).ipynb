{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, Conv1D, GlobalMaxPooling1D\n",
    "# from generate_train_test_datasets import load_pickle, save_as_pickle, generate_text_graph\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# from evaluate_results import evaluate_model_results\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Activation, Dense, Reshape \n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from keras.backend import clear_session\n",
    "from keras.initializers import Constant\n",
    "from nltk.tokenize import word_tokenize\n",
    "from threading import current_thread\n",
    "from argparse import ArgumentParser\n",
    "from collections import OrderedDict\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from keras import backend as K\n",
    "from functools import partial\n",
    "import torch.optim as optim\n",
    "from numpy import save,load\n",
    "import concurrent.futures\n",
    "from numpy import asarray\n",
    "from keras import layers\n",
    "# from models import gcn\n",
    "from keras import Model\n",
    "from tqdm import tqdm\n",
    "import esprima as esp\n",
    "from time import time\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import gensim \n",
    "import random\n",
    "import urllib\n",
    "import pickle\n",
    "import torch\n",
    "import pydot\n",
    "import math\n",
    "import html\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "np.random.seed(123)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'On JavaScript File'\n",
    "# 'On Payload File'\n",
    "par_path = \"/content/drive/MyDrive/Colab Notebooks/Muawiya/Js_Contana/On JavaScript File/Par\"\n",
    "model_info_path = \"/content/drive/MyDrive/Colab Notebooks/Muawiya/Js_Contana/On JavaScript File/Model info\"\n",
    "results_path = \"/content/drive/MyDrive/Colab Notebooks/Muawiya/Js_Contana/On JavaScript File/Results\"\n",
    "data_path = \"/content/drive/MyDrive/Colab Notebooks/Muawiya/Js_Contana/On JavaScript File/DataSets\"\n",
    "codes_path = \"/content/drive/MyDrive/Colab Notebooks/Muawiya/Js_Contana/On JavaScript File/Codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Callback To Include in Callbacks List At Training Time\n",
    "class GarbageCollectorCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyunpack\n",
    "#!pip install patool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # result = result.sample(frac=1)\n",
    "#from pyunpack import Archive\n",
    "#Archive(\"/content/drive/MyDrive/Colab Notebooks/Muawiya/df_0.rar\" ).extractall(\"/content/drive/MyDrive/Colab Notebooks/Muawiya\")\n",
    "#Archive(\"/content/drive/MyDrive/Colab Notebooks/Muawiya/df_1.rar\" ).extractall(\"/content/drive/MyDrive/Colab Notebooks/Muawiya\")\n",
    "#df0 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Muawiya/df_0.csv')\n",
    "#df1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Muawiya/df_1.csv')\n",
    "#frames = [df0, df1]\n",
    "#result = pd.concat(frames)\n",
    "#result = shuffle(result)\n",
    "#result.to_csv(\"/content/drive/MyDrive/Colab Notebooks/Muawiya/XSS_dataset_esprima.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Muawiya/XSS_dataset_esprima.csv\")\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.shape)\n",
    "#print(df[df['Label']==1].shape)\n",
    "#print(df[df['Label']==0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 500000\n",
    "num_words = 400000\n",
    "maxlen = 2048\n",
    "embedding_dim = 512\n",
    "myoptimizer = 'adam'\n",
    "myloss= 'binary_crossentropy'\n",
    "mymetrics = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename,path):\n",
    "    \"\"\"\n",
    "    _ INPUT (obj) THE OBJECT WE NEED SAVW IT (filename) THE NAME OF OBJECT\n",
    "    \"\"\"\n",
    "    filename = os.path.join(path,filename)\n",
    "    with open(filename+\".pkl\", 'wb') as outp:\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    outp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_object(filename,path):\n",
    "    \"\"\"\n",
    "    _ INPUT THE NAME OF OBJECT WE NEED LOAD IT\n",
    "    \"\"\"\n",
    "    filename = os.path.join(path,filename)\n",
    "    with open(filename+\".pkl\", 'rb') as outp:\n",
    "        loaded_object = pickle.load(outp)\n",
    "    outp.close()\n",
    "    return loaded_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowering_remove_url_ip_(script):\n",
    "    \"\"\"\n",
    "    _ INPUT IS JS FILE \n",
    "    _ OUTPUT IS CLEAN JS FILE\n",
    "    \"\"\"\n",
    "    script = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', script)\n",
    "    script = re.sub(r'((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)', '', script)\n",
    "    script = re.sub(r\"(/\\*([^*]|[\\r\\n]|(\\*+([^*/]|[\\r\\n])))*\\*+/)|(//.*)\",'',script)\n",
    "    script = script.lower()\n",
    "    return script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_dataset_(scripts,fit = True):\n",
    "    \"\"\"\n",
    "    _INPUT (scripts) JS FILES (fit) IF THE FILES IS FROM TRANING SET IT WILL BE (fit=True) ELSE IT WILL BE (fit=False)\n",
    "    \"\"\"\n",
    "    if fit:\n",
    "        tokenizer = Tokenizer(num_words=num_words) #400000\n",
    "        tokenizer.fit_on_texts(scripts)\n",
    "        our_tokenizer = save_object(tokenizer, \"our_tokenizer\")\n",
    "    else :\n",
    "        tokenizer = load_object(\"our_tokenizer\")\n",
    "    X = tokenizer.texts_to_sequences(scripts)\n",
    "    X = pad_sequences(X, padding='post', maxlen=maxlen) #2048\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_of_syntax_units(scripts):\n",
    "    \"\"\"\n",
    "    sctipts : list or single javascript code \n",
    "    return sequence of syntax units\n",
    "    each syntax unit correspond to line in an abstract syntax tree\n",
    "    \"\"\"\n",
    "    if type(scripts) is list:\n",
    "        # esp.parseScript(script) returns abstract syntax tree of each js scripts\n",
    "        return [re.sub('\\s+', ' ', ''.join(str(esp.parseScript(script)).split('\\n'))) for script in scripts]\n",
    "    elif type(scripts) is str:\n",
    "        return re.sub('\\s+', ' ', ''.join(str(esp.parseScript(scripts)).split('\\n')))\n",
    "    else:\n",
    "        raise ValueError('The type of scripts parameter must be {list or string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(x_chunk,y_chunk):\n",
    "    \"\"\"\n",
    "    _INPUT (x_chunk) SET OF JS CODE AFTER APPLY PREPROCESSING DATASET FUNCTION (y_chunk)\n",
    "    \"\"\"\n",
    "    indexes = random.sample(range(3, x_chunk.shape[0]), random.randint(1, x_chunk.shape[0]-3))\n",
    "    x_chunk[indexes,:,:] = 0\n",
    "    return x_chunk,y_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model,y_predict,y_test):\n",
    "    \"\"\"\n",
    "    _INPUT \n",
    "    (model) THE MODEL THAT WE NEED EVALUATE\n",
    "    (chunk_number) NUMBER OF CHUNK FROM DATA WE PROCESS\n",
    "    (history) IF WE NEED CALL FUNCTION plot_history\n",
    "    (X_train,y_train,X_test,y_test) TRAINING DATA SET AND TESTING DATA SET\n",
    "    (model_comparison_table) DECTIONARY TO SAVE RESULT [train_loss, train_accuracy,test_loss, test_accuracy,Classification_report]\n",
    "    \"\"\"\n",
    "    Classification_report = classification_report(y_predict,y_test)\n",
    "    return Classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(shape=None,with_first_layer=False):\n",
    "    \"\"\"\n",
    "    _INPUT (down) IF WE WANT TO CREAT MODEL WITHOUT FIRST LSTM LAYER  (shape) IF THE PREVIOS PAREMETER IS TRUE WE NEED DIFINE THE SHAPE FOR INPUT LAYER\n",
    "    _OUTPUT THE MODEL \n",
    "    \"\"\"\n",
    "    model = Sequential() \n",
    "    if with_first_layer:\n",
    "        model.add(layers.InputLayer(input_shape=shape))\n",
    "        model.add(Bidirectional(LSTM(units=50, input_shape=(None, 50), return_sequences=True)))\n",
    "        # TextCNN with 4 conv layers\n",
    "        model.add(Conv1D(128, 7, activation='tanh', input_shape=(None, 32)))\n",
    "        model.add(Conv1D(128, 15, activation='tanh'))\n",
    "        model.add(Conv1D(128, 25, activation='tanh'))\n",
    "        model.add(Conv1D(128, 35, activation='tanh'))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        os.path.join(model_info_path,'weights.best.hdf5')\n",
    "        #model.load_weights(os.path.join(model_info_path,'with_first_layer_weights.best.hdf5'))\n",
    "    else:\n",
    "        model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "        model.add(Bidirectional(LSTM(units=50, input_shape=(None, 50), return_sequences=True))) \n",
    "        model.add(Bidirectional(LSTM(units=50, input_shape=(None, 50), return_sequences=True)))\n",
    "        # TextCNN with 4 conv layers\n",
    "        model.add(Conv1D(128, 7, activation='tanh', input_shape=(None, 32)))\n",
    "        model.add(Conv1D(128, 15, activation='tanh'))\n",
    "        model.add(Conv1D(128, 25, activation='tanh'))\n",
    "        model.add(Conv1D(128, 35, activation='tanh'))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        #model.load_weights(os.path.join(model_info_path,'without_first_layer_weights.best.hdf5'))\n",
    "    model.compile(optimizer=myoptimizer,loss=myloss,metrics=[mymetrics])\n",
    "    #model.summary()\n",
    "    #plot_model(model, show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_metrics():\n",
    "    model_comparison_table = {}\n",
    "    model_comparison_table['chunk_number'] = []\n",
    "    model_comparison_table['y_test'] = []\n",
    "    model_comparison_table['y_predict'] = []\n",
    "    return model_comparison_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputer(model_comparison_table,chunk_nuber,y_predict,y_test):\n",
    "    #y_predict[y_predict>0.5] = 1\n",
    "    model_comparison_table['chunk_number'].append(\"chunk_number \"+str(chunk_nuber))\n",
    "    model_comparison_table['y_test'].append(y_test)\n",
    "    model_comparison_table['y_predict'].append(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking_data(Data,test_size=0.20,train_size=0.80,validation_size=0.0):\n",
    "    \"\"\"\n",
    "    _INPUT (Data) CHUNK OF DATASET (validation_size=0.10,test_size=0.20,train_size=0.70) SIZE OF SPLITNG\n",
    "    _OUTPUT (X_train , y_train , X_test , y_test , X_validation , y_validation)\n",
    "    \"\"\"\n",
    "    test_data = Data.iloc[:int((Data.shape[0]*test_size))]\n",
    "    \n",
    "    # validation_data = Data.iloc[int((Data.shape[0]*test_size)):(int((Data.shape[0]*validation_size))+int((Data.shape[0]*test_size)))]\n",
    "    \n",
    "    train_data = Data.iloc[int((Data.shape[0]*test_size)):]    # +int((Data.shape[0]*validation_size))):]\n",
    "    \n",
    "    X_test = test_data[test_data.columns[-2]]\n",
    "    y_test = test_data[test_data.columns[-1]]\n",
    "    \n",
    "    # X_validation = validation_data[validation_data.columns[-2]]\n",
    "    # y_validation = validation_data[validation_data.columns[-1]]\n",
    "    \n",
    "    X_train = train_data[train_data.columns[-2]]\n",
    "    y_train = train_data[train_data.columns[-1]]\n",
    "    \n",
    "    return X_train , y_train , X_test , y_test #, X_validation , y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read__script(script_paths,parent_path,y):\n",
    "    x_temp = []\n",
    "    y_temp = []\n",
    "    todrop = []\n",
    "    for path in script_paths:\n",
    "        try:\n",
    "            with open(os.path.join(parent_path,path), 'r') as js_file: # , encoding='utf-8'\n",
    "                script = js_file.read()\n",
    "            js_file.close()\n",
    "#             script = lowering_remove_url_ip_(script)\n",
    "            script = sequence_of_syntax_units(script)\n",
    "            new_y = y[script_paths.index(path)]\n",
    "            x_temp.append(script)\n",
    "            y_temp.append(new_y)\n",
    "        except:\n",
    "            pass\n",
    "    if len(x_temp)>0:\n",
    "        return x_temp , np.array(y_temp)\n",
    "    else:\n",
    "        return None , None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ScriptPath</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>javascript-malware-collection-master\\2016\\2016...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>javascript-malware-collection-master\\2016\\2016...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>javascript-malware-collection-master\\2016\\2016...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>javascript-malware-collection-master\\2016\\2016...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>javascript-malware-collection-master\\2016\\2016...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          ScriptPath  Label\n",
       "0  javascript-malware-collection-master\\2016\\2016...      1\n",
       "1  javascript-malware-collection-master\\2016\\2016...      1\n",
       "2  javascript-malware-collection-master\\2016\\2016...      1\n",
       "3  javascript-malware-collection-master\\2016\\2016...      1\n",
       "4  javascript-malware-collection-master\\2016\\2016...      1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df = pd.read_csv(\"G:\\JSContanaDataSet\\my_df_1.csv\")\n",
    "print(my_df.shape)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ScriptPath</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data\\tsukhu\\rwd-spa-alljs-app\\routes\\user.js</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data\\dataseed\\dataseed-visualisation.js\\src\\js...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data\\node-app\\Nodelike\\Nodelike Tests\\package\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data\\ethereum\\ethereum.js\\test\\utils.toDecimal.js</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data\\cdnjs\\cdnjs\\ajax\\libs\\webshim\\1.14.3-RC2\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          ScriptPath  Label\n",
       "0       data\\tsukhu\\rwd-spa-alljs-app\\routes\\user.js      0\n",
       "1  data\\dataseed\\dataseed-visualisation.js\\src\\js...      0\n",
       "2  data\\node-app\\Nodelike\\Nodelike Tests\\package\\...      0\n",
       "3  data\\ethereum\\ethereum.js\\test\\utils.toDecimal.js      0\n",
       "4  data\\cdnjs\\cdnjs\\ajax\\libs\\webshim\\1.14.3-RC2\\...      0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df = pd.read_csv(\"G:\\JSContanaDataSet\\my_df_0.csv\")\n",
    "print(my_df.shape)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read__script(script_paths,parent_path):\n",
    "    try :\n",
    "        with open(os.path.join(parent_path,script_paths), 'r') as js_file:\n",
    "            script = js_file.read()\n",
    "#             script = sequence_of_syntax_units(script)\n",
    "        return script\n",
    "    except:\n",
    "        print(script_paths)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result_table = creat_metrics()\n",
    "bottom_model_result_table = creat_metrics()\n",
    "hiden_state = {}\n",
    "hiden_state['chunk_number'] = []\n",
    "hiden_state['hiden_state'] = []\n",
    "\n",
    "model_is_created = False\n",
    "model_tow_is_created = False\n",
    "\n",
    "filepath_1 = os.path.join(model_info_path,'without_first_layer_weights.best.hdf5')\n",
    "checkpoint_1 = ModelCheckpoint(filepath_1, monitor='val_accuracy', verbose=False, save_best_only=True, mode='max')\n",
    "es_1 = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "callbacks_list_1 = [GarbageCollectorCallback(),checkpoint_1, es_1]\n",
    "\n",
    "filepath_2 =os.path.join(model_info_path,'with_first_layer_weights.best.hdf5')\n",
    "checkpoint_2 = ModelCheckpoint(filepath_2, monitor='val_accuracy', verbose=False, save_best_only=True, mode='max')\n",
    "es_2 = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "callbacks_list_2 = [GarbageCollectorCallback(),checkpoint_2, es_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df_iterator in enumerate(pd.read_csv(os.path.join(datasets_path,'-----'),chunksize=2000)):\n",
    "    print(\"===============================\"+str(i)+\"=======================================\")\n",
    "    X_train , y_train , X_test , y_test  = chunking_data(df_iterator)\n",
    "    del df_iterator\n",
    "    # X_train , y_train = read__script(X_train.tolist(),\"/content/drive/MyDrive/Colab Notebooks\",y_train.tolist())\n",
    "    # X_test , y_test = read__script(X_test.tolist(),\"/content/drive/MyDrive/Colab Notebooks/\",y_test.tolist())\n",
    "    # X_validation , y_validation = read__script(X_validation.tolist(),\"/content/drive/MyDrive/Colab Notebooks/\",y_validation.tolist())\n",
    "    X_train = preprocessing_dataset_(X_train,fit = True)\n",
    "    X_test = preprocessing_dataset_(X_test,fit=False)\n",
    "    if not model_is_created:\n",
    "        model = create_model() \n",
    "        # model.save(os.path.join(model_info_path,\"model chunk \"+str(i)+\".h5\"))\n",
    "          model_is_created = True\n",
    "    # else:\n",
    "      # model = load_model(os.path.join(model_info_path,\"model chunk \"+str(i-1)+\".h5\"))\n",
    "    \n",
    "    history_Model = model.fit(X_train, y_train, epochs=10,  validation_split=0.20,callbacks=callbacks_list_1)\n",
    "    \n",
    "    top_model = Model(inputs=model.input,outputs=model.layers[1].output)\n",
    "    inputer(model_result_table,i,model.predict(X_test),y_test)\n",
    "    # model.save(os.path.join(model_info_path,\"model chunk \"+str(i)+\".h5\"))\n",
    "    keras_function_train = K.function([model.input], [model.layers[1].output])\n",
    "    X_train = keras_function_train([X_train, 1])[0]\n",
    "    save_object(X_train, \"X_train_hiden_for_chunk \"+str(i))\n",
    "    X_test = top_model.predict(X_test)\n",
    "    save_object(X_test, \"X_test_hiden_for_chunk \"+str(i))\n",
    "    X_train , y_train = feature_selection(X_train,y_train)\n",
    "    if not model_tow_is_created:\n",
    "        bottom_model = create_model(shape=X_train[0].shape,down=True)\n",
    "        os.path.join(model_info_path,\"bottom_model chunk \"+str(i)+\".h5\")\n",
    "        # bottom_model.save(os.path.join(model_info_path,\"bottom_model chunk \"+str(i)+\".h5\"))\n",
    "        model_tow_is_created = True \n",
    "    # else:\n",
    "        # bottom_model = load_model(os.path.join(model_info_path,\"bottom_model chunk \"+str(i-1)+\".h5\"))\n",
    "    history_Model_bottom_model = bottom_model.fit(X_train, y_train, epochs=10,  validation_split=0.20,callbacks=callbacks_list_2)\n",
    "    inputer(bottom_model_result_table,i,bottom_model.predict(X_test),y_test)\n",
    "    # bottom_model.save(os.path.join(model_info_path,\"bottom_model chunk \"+str(i)+\".h5\"))\n",
    "    save_object(model_result_table, \"model_result_table \"+str(i))\n",
    "    save_object(bottom_model_result_table, \"bottom_model_result_table \"+str(i))\n",
    "model.save(os.path.join(model_info_path,\"model chunk.h5\"))\n",
    "bottom_model.save(os.path.join(model_info_path,\"bottom_model chunk.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result_table = load_object(\"model_result_table\")\n",
    "bottom_model_result_table =load_object(\"bottom_model_result_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(model_result_table)\n",
    "df_ = pd.DataFrame(bottom_model_result_table)\n",
    "df_.to_csv(os.path.join(data_path,\"bottom_model_result_table.csv\"),index=False)\n",
    "df.to_csv(os.path.join(data_path,\"model_result_table.csv\"),index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    print(\"    \" + empty_cell, end=\" \")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "    print()\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replaceitem(x):\n",
    "    if type(x) is list:\n",
    "        if x[0]<0.5:\n",
    "            return 0.0\n",
    "    else:\n",
    "        x = float(x)\n",
    "        if x<0.5:\n",
    "            return 0.0\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_functions = {'accuracy': accuracy_score, 'precision': precision_score, \n",
    "                        'recall': recall_score, 'f1_score': f1_score}\n",
    "def cummulative_metrics(y_true, y_pred, step_size, metric):\n",
    "    metric_score = []\n",
    "    for i in range(step_size, len(y_true) - step_size, step_size):\n",
    "        metric_score.append(metric_functions[metric](y_true[:i], y_pred[:i]))\n",
    "    return metric_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_plot(df,  metric, step_size=40, title=\" \", method_names=None, drift_data_path=None):\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    colors = ['r', 'g', 'b', 'm', 'c']\n",
    "    y_true, y_pred = None,None\n",
    "    for d in range(df.shape[0]):\n",
    "        if y_true is None:\n",
    "            y_true, y_pred = df['y_test'][d].tolist() , np.concatenate(df['y_predict'].tolist())\n",
    "            y_true = list(map(_replaceitem, y_true))\n",
    "            y_pred = list(map(_replaceitem, y_pred))\n",
    "        else:\n",
    "            y_true = y_true + df['y_test'][d].tolist()\n",
    "            y_pred = y_pred + list(np.concatenate(df['y_predict'][d].tolist()))\n",
    "            y_true = list(map(_replaceitem, y_true))\n",
    "            y_pred = list(map(_replaceitem, y_pred))\n",
    "    metric_score = cummulative_metrics(y_true, y_pred, step_size, metric)\n",
    "    X = np.linspace(0, 16000, len(metric_score))\n",
    "    plt.plot(X, metric_score, label='{}'.format(title))\n",
    "    plt.xlabel(\"Time Series\")\n",
    "    plt.ylabel(\"{}\".format(metric))\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(os.path.join(results_path,\"Image\"),\"{}_{}.png\".format(title, metric)))\n",
    "    plt.savefig(os.path.join(os.path.join(results_path,\"Image\"),\"{}_{}.svg\".format(title, metric)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['accuracy','precision','recall','f1_score']\n",
    "for metric in metrics:\n",
    "    time_series_plot(df,  metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    time_series_plot(df_,  metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(y_pred, y_test , title ,color):\n",
    "    if len(y_test) < len(y_pred):\n",
    "        y_pred = y_pred[: len(y_test)]\n",
    "    elif len(y_test) > len(y_pred):\n",
    "        y_test = y_test[: len(y_pred)]\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "    con_matrix = confusion_matrix(y_test, y_pred)\n",
    "    if len(con_matrix)==1:\n",
    "        if len(con_matrix[0])==1:\n",
    "            con_matrix = list(con_matrix)\n",
    "            con_matrix[0] = list(con_matrix[0])\n",
    "            con_matrix[0].append(0)\n",
    "            con_matrix = np.array([con_matrix[0],[0,0]])\n",
    "    plt.figure(figsize=(25,15))\n",
    "    plt.subplot(2,1,1)\n",
    "    \n",
    "    T1 = y_test.count(0) if y_test.count(0)!=0 else 1\n",
    "    T2 = y_test.count(1) if y_test.count(1)!=0 else 1\n",
    "    \n",
    "    group_counts = con_matrix\n",
    "    group_percentages = np.round(con_matrix / np.array([[T1 , T1],[T2 , T2]]),3)\n",
    "    t = 0.5\n",
    "    plt.text(x=0.0 , y = 2.6, s =  \"confusion_matrix :\",fontsize=15)\n",
    "    plt.text(x=0.0 , y = 3.3-t, s =  \"Zero \",fontsize=15)\n",
    "    plt.text(x=0.0 , y = 3.4-t, s =  \"One \",fontsize=15)\n",
    "    plt.text(x=0.3 , y = 3.2-t, s =  \"Zero\",fontsize=15)\n",
    "    plt.text(x=0.6 , y = 3.2-t, s =  \"One\",fontsize=15)\n",
    "    plt.text(x=0.3 , y = 3.3-t, s =  \"{}\".format(con_matrix[0][0]),fontsize=15)\n",
    "    plt.text(x=0.6 , y = 3.3-t, s =  \"{}\".format(con_matrix[0][1]),fontsize=15)\n",
    "    plt.text(x=0.3 , y = 3.4-t, s =  \"{}\".format(con_matrix[1][0]),fontsize=15)\n",
    "    plt.text(x=0.6 , y = 3.4-t, s =  \"{}\".format(con_matrix[1][1]),fontsize=15)\n",
    "  \n",
    "    sns.heatmap(group_percentages,xticklabels= ['Zero' , 'One'], yticklabels=['Zero' , 'One']  , annot=True, annot_kws={\"size\": 16},fmt='g', cmap=color)\n",
    "    plt.title(title ,  fontsize = 18)\n",
    "    plt.text(x =0,y = 2.3 , s = \"accuracy  score on test : {}\".format(accuracy),fontsize=15)    \n",
    "    plt.text(x=0 , y = 3.9, s =  \"classification_report :\\n {}\".format(classification_rep),fontsize=15)\n",
    "    plt.xlabel(\"Predicted value\")\n",
    "    plt.ylabel(\"Real value\")\n",
    "    plt.savefig(os.path.join(os.path.join(results_path,\"/Image\"),\"{} .png\".format(title)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,y_test in enumerate(df['y_test'].tolist()):\n",
    "    y_test = list(map(_replaceitem, y_test))\n",
    "    y_predict = list(map(_replaceitem, df['y_predict'][i].tolist()))\n",
    "    print_result(y_predict, y_test , \"chunk numer \"+str(i) ,'Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,y_test in enumerate(df_['y_test'].tolist()):\n",
    "    y_test = list(map(_replaceitem, y_test))\n",
    "    y_predict = list(map(_replaceitem, df_['y_predict'][i].tolist()))\n",
    "    print_result(y_predict, y_test , \"_chunk numer \"+str(i) ,'Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
