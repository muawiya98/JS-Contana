{"cells":[{"cell_type":"code","source":["# !pip install tensorflow==2.3.1\n","# !pip install q keras==2.4.3\n","# !pip install numpy==1.18.1\n","# !pip install pandas==1.0.1\n","!pip install esprima"],"metadata":{"id":"BTtnyvyLWN4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, precision_recall_curve, classification_report, confusion_matrix\n","from keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, Conv1D, GlobalMaxPooling1D\n","from keras.callbacks import ModelCheckpoint, EarlyStopping,Callback,CSVLogger\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.feature_extraction.text import CountVectorizer\n","from keras.layers import Activation, Dense, Reshape, Input\n","from sklearn.model_selection import train_test_split,KFold\n","from sklearn.preprocessing import StandardScaler\n","from keras.models import Sequential, load_model\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import Model , clone_model\n","from keras.losses import BinaryCrossentropy\n","from keras.backend import clear_session\n","from keras.initializers import Constant\n","from nltk.tokenize import word_tokenize\n","from threading import current_thread\n","from keras.utils import plot_model\n","from sklearn.utils import shuffle\n","from keras.optimizers import Adam\n","from keras.utils import Sequence\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from keras import backend as K\n","from functools import partial\n","from numpy import asarray\n","import concurrent.futures\n","from keras import layers\n","import matplotlib as mpl\n","from keras import Model\n","from numpy import save\n","from numpy import load\n","import seaborn as sns\n","import esprima as esp\n","from time import time\n","import pandas as pd\n","import numpy as np\n","import tempfile\n","import logging\n","import urllib\n","import random\n","import pickle\n","import pydot\n","import torch\n","import math\n","import html\n","import nltk\n","import os\n","import re\n","import gc\n","np.random.seed(123)\n","plt.style.use('ggplot')\n","# mpl.rcParams['figure.figsize'] = (12, 10)\n","colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"],"metadata":{"id":"QZ2ymXUqWOeQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxgWX6jZGVC7"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","source":["par_path = \"/content/drive/MyDrive/Colab Notebooks/Muawiya/Js_Contana/On JavaScript File/Hiden_Co\"\n","model_info_path = \"/content/drive/MyDrive/Colab Notebooks/Muawiya/Js_Contana/On JavaScript File/Results/Model info\"\n","results_path = \"/content/drive/MyDrive/Colab Notebooks/Muawiya/Js_Contana/On JavaScript File/Results\"\n","data_path = \"/content/drive/MyDrive/Colab Notebooks/Muawiya/Js_Contana/On JavaScript File/DataSets\""],"metadata":{"id":"qWIQ7HS1l2h0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKcTOwH2GVC8"},"outputs":[],"source":["class GarbageCollectorCallback(Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hg_OhoRtGVC-"},"outputs":[],"source":["def save_object(obj, filename,path):\n","    \"\"\"\n","    _ INPUT (obj) THE OBJECT WE NEED SAVW IT (filename) THE NAME OF OBJECT\n","    \"\"\"\n","    filename = os.path.join(path,filename)\n","    with open(filename+\".pkl\", 'wb') as outp:\n","        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n","    outp.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W8sQUojTGVC_"},"outputs":[],"source":["def load_object(filename,path):\n","    \"\"\"\n","    _ INPUT THE NAME OF OBJECT WE NEED LOAD IT\n","    \"\"\"\n","    filename = os.path.join(path,filename)\n","    with open(filename+\".pkl\", 'rb') as outp:\n","        loaded_object = pickle.load(outp)\n","    outp.close()\n","    return loaded_object"]},{"cell_type":"code","source":["def Pad_Sequences(scripts,fit = True,creat=True):\n","    if fit:\n","        tokenizer = Tokenizer(num_words=num_words)\n","        tokenizer.fit_on_texts(scripts)\n","        save_object(tokenizer, \"Pad_Sequences_tokenizer\",results_path)\n","    else :\n","        tokenizer = load_object(\"Pad_Sequences_tokenizer\",results_path)\n","    X = tokenizer.texts_to_sequences(scripts)\n","    X = pad_sequences(X, padding='post', maxlen=maxlen)\n","    return X"],"metadata":{"id":"S8rL16qkxWhW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqGeTzVaGVDD"},"outputs":[],"source":["def chunking_data(Data,test_size=0.20):\n","    \"\"\"\n","    _INPUT (Data) CHUNK OF DATASET (test_size=0.20) SIZE OF SPLITNG\n","    _OUTPUT (X_train , y_train , X_test , y_test)\n","    \"\"\"\n","    train_data, test_data = train_test_split(Data, test_size=0.2,random_state=42,shuffle=False)\n","    X_test = test_data[test_data.columns[-2]]\n","    y_test = test_data[test_data.columns[-1]]\n","    X_train = train_data[train_data.columns[-2]]\n","    y_train = train_data[train_data.columns[-1]]\n","    return X_train , y_train , X_test , y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oevNVDbOGVDE"},"outputs":[],"source":["num_words = 400000\n","maxlen = 2048\n","a = np.array([[1, 2], [3, 4]])\n","b = np.array([[5, 6],[5,5]])\n","model = load_model(filepath=os.path.join(model_info_path,\"model1.h5\"))"]},{"cell_type":"code","source":["for i,df_iterator in tqdm(enumerate(pd.read_csv(os.path.join(data_path,\"XSS_dataset_js.csv\"),chunksize=2000))):\n","  df_iterator = df_iterator.mask(df_iterator.eq(np.nan)).dropna()\n","  X_train , y_train , X_test , y_test  = chunking_data(df_iterator)\n","  X_train = Pad_Sequences(X_train,fit = True)\n","  X_test = Pad_Sequences(X_test,fit=False) # ,creat=False\n","\n","  sub_model_1 = Model(inputs=model.input,outputs=model.layers[1].output)\n","  sub_model_2 = Model(inputs=model.input,outputs=model.layers[2].output)\n","\n","  X_train_1 = sub_model_1.predict(X_train)\n","  X_train_2 = sub_model_1.predict(X_train)\n","\n","  X_test_1 = sub_model_1.predict(X_test)\n","  X_test_2 = sub_model_2.predict(X_test)\n","\n","  Train_Dynamic_word_embeding = []\n","  for j in range(X_train_1.shape[0]):\n","    temp = np.concatenate((X_train_1[i], X_train_2[i]), axis=1)\n","    Train_Dynamic_word_embeding.append(np.concatenate((temp, np.reshape(X_train[i],(X_train[i].shape[0],1))), axis=1))\n","  \n","  Test_Dynamic_word_embeding = []\n","  for j in range(X_test_1.shape[0]):\n","    temp = np.concatenate((X_test_1[i], X_test_2[i]), axis=1)\n","    Test_Dynamic_word_embeding.append(np.concatenate((temp, np.reshape(X_test[i],(X_test[i].shape[0],1))), axis=1))\n","    \n","\n","  save_object(np.array(Train_Dynamic_word_embeding), \"X_train_co_hiden_for_chunk_\"+str(i),par_path)\n","  save_object(y_train, \"y_train_co_hiden_for_chunk_\"+str(i),par_path)\n","  \n","  save_object(np.array(Test_Dynamic_word_embeding), \"X_test_co_hiden_for_chunk_\"+str(i),par_path)\n","  save_object(y_test, \"y_test_co_hiden_for_chunk_\"+str(i),par_path)"],"metadata":{"id":"V7YsxXgHCqvR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5ANo6qKkoSFG"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[],"machine_shape":"hm"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}