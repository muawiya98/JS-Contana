{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5WsBnKyPFMa",
        "outputId": "ce444aea-1415-400c-b459-fb3b174d6a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==2.3.1 in /usr/local/lib/python3.7/dist-packages (2.3.1)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.14.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (2.0.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.37.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.3.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.49.1)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.18.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (5.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.2.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: q in /usr/local/lib/python3.7/dist-packages (2.7)\n",
            "Requirement already satisfied: keras==2.4.3 in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (6.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (1.18.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.4.3) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy==1.18.1 in /usr/local/lib/python3.7/dist-packages (1.18.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas==1.0.1 in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.1) (2022.4)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.1) (1.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.6.1->pandas==1.0.1) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: esprima in /usr/local/lib/python3.7/dist-packages (4.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.3.1\n",
        "!pip install q keras==2.4.3\n",
        "!pip install numpy==1.18.1\n",
        "!pip install pandas==1.0.1\n",
        "!pip install esprima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXCUlR9rx4o6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.backend import clear_session\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Activation, Dense, Reshape \n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "from keras.models import load_model\n",
        "from numpy import asarray\n",
        "from numpy import save\n",
        "from numpy import load\n",
        "from sklearn.metrics import classification_report , confusion_matrix\n",
        "import esprima as esp\n",
        "import re\n",
        "from keras.initializers import Constant\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import BinaryCrossentropy\n",
        "from keras.utils import plot_model\n",
        "from keras import Model\n",
        "from keras.utils import Sequence\n",
        "import pydot\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pickle\n",
        "from keras import backend as K\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import logging\n",
        "from time import time\n",
        "from functools import partial\n",
        "import concurrent.futures\n",
        "from threading import current_thread\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "from keras.layers import Input\n",
        "from keras.models import Model , clone_model\n",
        "from sklearn.utils import shuffle\n",
        "import gc\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "np.random.seed(123)\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeuNp9L-7i4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd59822-669e-4f4b-eb81-32a1f348a2ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Callback To Include in Callbacks List At Training Time\n",
        "class GarbageCollectorCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()"
      ],
      "metadata": {
        "id": "bBYIVhg0ptk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBa0Ba6jpDsh"
      },
      "outputs": [],
      "source": [
        "#!pip install pyunpack\n",
        "#!pip install patool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TeVqunkohMf"
      },
      "outputs": [],
      "source": [
        "# # result = result.sample(frac=1)\n",
        "\n",
        "#from pyunpack import Archive\n",
        "#Archive(\"/content/drive/MyDrive/Colab Notebooks/Muawiya/df_0.rar\" ).extractall(\"/content/drive/MyDrive/Colab Notebooks/Muawiya\")\n",
        "#Archive(\"/content/drive/MyDrive/Colab Notebooks/Muawiya/df_1.rar\" ).extractall(\"/content/drive/MyDrive/Colab Notebooks/Muawiya\")\n",
        "#df0 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Muawiya/df_0.csv')\n",
        "#df1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Muawiya/df_1.csv')\n",
        "#frames = [df0, df1]\n",
        "#result = pd.concat(frames)\n",
        "#result = shuffle(result)\n",
        "#result.to_csv(\"/content/drive/MyDrive/Colab Notebooks/Muawiya/XSS_dataset_esprima.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Muawiya/XSS_dataset_esprima.csv\")\n",
        "#df.head()"
      ],
      "metadata": {
        "id": "ZB9SaGev4hPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df.shape"
      ],
      "metadata": {
        "id": "eAr50PE04-D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(df[df['Label']==1].shape)\n",
        "#print(df[df['Label']==0].shape)"
      ],
      "metadata": {
        "id": "vvBzCFzZ3UIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkgxeec8fEjT"
      },
      "source": [
        "Hyperparameter for Our Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5OTb7eq5ylf"
      },
      "outputs": [],
      "source": [
        "vocab_size = 500000\n",
        "# 154756\n",
        "num_words = 400000\n",
        "maxlen = 2048\n",
        "embedding_dim = 512\n",
        "myoptimizer = 'adam'\n",
        "myloss= 'binary_crossentropy'\n",
        "mymetrics = 'accuracy'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0csThO54fsJV"
      },
      "source": [
        "save_object & load_object to save and load All objects we are need it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuB9VYRXPFMf"
      },
      "outputs": [],
      "source": [
        "def save_object(obj, filename):\n",
        "    \"\"\"\n",
        "    _ INPUT (obj) THE OBJECT WE NEED SAVW IT (filename) THE NAME OF OBJECT\n",
        "    \"\"\"\n",
        "    filename = os.path.join('/content/drive/MyDrive/Colab Notebooks/Muawiya/results/par',filename)\n",
        "    with open(filename+\".pkl\", 'wb') as outp:\n",
        "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
        "    outp.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w32njEwEPFMf"
      },
      "outputs": [],
      "source": [
        "def load_object(filename):\n",
        "    \"\"\"\n",
        "    _ INPUT THE NAME OF OBJECT WE NEED LOAD IT\n",
        "    \"\"\"\n",
        "    filename = os.path.join('/content/drive/MyDrive/Colab Notebooks/Muawiya/results/par',filename)\n",
        "    with open(filename+\".pkl\", 'rb') as outp:\n",
        "        loaded_object = pickle.load(outp)\n",
        "    outp.close()\n",
        "    return loaded_object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxEPSqgNPFMg"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp3MOabsgYmF"
      },
      "source": [
        "lowering_remove_url_ip_: this function used to cleaning js scripts \n",
        "\n",
        "\n",
        "1.   remove urls\n",
        "2.   remove ips\n",
        "3.   remove comments\n",
        "4.   convert to lower case\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ko40zJGqnCW"
      },
      "outputs": [],
      "source": [
        "def lowering_remove_url_ip_(script):\n",
        "    \"\"\"\n",
        "    _ INPUT IS JS FILE \n",
        "    _ OUTPUT IS CLEAN JS FILE\n",
        "    \"\"\"\n",
        "    script = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', script)\n",
        "    script = re.sub(r'((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)', '', script)\n",
        "    script = re.sub(r\"(/\\*([^*]|[\\r\\n]|(\\*+([^*/]|[\\r\\n])))*\\*+/)|(//.*)\",'',script)\n",
        "    script = script.lower()\n",
        "    return script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdvPs5n9kooY"
      },
      "source": [
        "preprocessing_dataset_: this function used to tokenzation js file and convert text to sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XqcEGYKPFMg"
      },
      "outputs": [],
      "source": [
        "def preprocessing_dataset_(scripts,fit = True):\n",
        "    \"\"\"\n",
        "    _INPUT (scripts) JS FILES (fit) IF THE FILES IS FROM TRANING SET IT WILL BE (fit=True) ELSE IT WILL BE (fit=False)\n",
        "    \"\"\"\n",
        "    if fit:\n",
        "        tokenizer = Tokenizer(num_words=num_words)\n",
        "        tokenizer.fit_on_texts(scripts)\n",
        "        our_tokenizer = save_object(tokenizer, \"our_tokenizer\")\n",
        "    else :\n",
        "        tokenizer = load_object(\"our_tokenizer\")\n",
        "    X = tokenizer.texts_to_sequences(scripts)\n",
        "    X = pad_sequences(X, padding='post', maxlen=maxlen)\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbQbuLePmnOH"
      },
      "source": [
        "sequence_of_syntax_units: this function used to convert java script code to the abstract syntax tree then extract the sequences of syntax units with detailed information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9BoJm8ZPFMh"
      },
      "outputs": [],
      "source": [
        "def sequence_of_syntax_units(scripts):\n",
        "    \"\"\"\n",
        "    sctipts : list or single javascript code \n",
        "    return sequence of syntax units\n",
        "    each syntax unit correspond to line in an abstract syntax tree\n",
        "    \"\"\"\n",
        "    if type(scripts) is list:\n",
        "        # esp.parseScript(script) returns abstract syntax tree of each js scripts\n",
        "        return [re.sub('\\s+', ' ', ''.join(str(esp.parseScript(script)).split('\\n'))) for script in scripts]\n",
        "    elif type(scripts) is str:\n",
        "        return re.sub('\\s+', ' ', ''.join(str(esp.parseScript(scripts)).split('\\n')))\n",
        "    else:\n",
        "        raise ValueError('The type of scripts parameter must be {list or string}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXUt70UAm-Wx"
      },
      "source": [
        "feature_selection: this function used to make initial feature selection on chunk of js file after used the preprocessing_dataset_ function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVR0u9kXPFMj"
      },
      "outputs": [],
      "source": [
        "def feature_selection(x_chunk,y_chunk):\n",
        "    \"\"\"\n",
        "    _INPUT (x_chunk) SET OF JS CODE AFTER APPLY PREPROCESSING DATASET FUNCTION (y_chunk)\n",
        "    \"\"\"\n",
        "    indexes = random.sample(range(3, x_chunk.shape[0]), random.randint(1, x_chunk.shape[0]-3))\n",
        "    x_chunk[indexes,:,:] = 0\n",
        "    return x_chunk,y_chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWBgODVloXX4"
      },
      "source": [
        "evaluation: this function used to evaluate our model and save result in comparison table "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZN229zLPFMj"
      },
      "outputs": [],
      "source": [
        "def evaluation(model,y_predict,y_test):\n",
        "    \"\"\"\n",
        "    _INPUT \n",
        "    (model) THE MODEL THAT WE NEED EVALUATE\n",
        "    (chunk_number) NUMBER OF CHUNK FROM DATA WE PROCESS\n",
        "    (history) IF WE NEED CALL FUNCTION plot_history\n",
        "    (X_train,y_train,X_test,y_test) TRAINING DATA SET AND TESTING DATA SET\n",
        "    (model_comparison_table) DECTIONARY TO SAVE RESULT [train_loss, train_accuracy,test_loss, test_accuracy,Classification_report]\n",
        "    \"\"\"\n",
        "    Classification_report = classification_report(y_predict,y_test)\n",
        "    return Classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2mXR50Sqn_2"
      },
      "source": [
        "create_model: this function used to creat neural network with 11 layer or with 9 layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9BXBE5JPFMk"
      },
      "outputs": [],
      "source": [
        "def create_model(shape=None,down=False):\n",
        "    \"\"\"\n",
        "    _INPUT (down) IF WE WANT TO CREAT MODEL WITHOUT FIRST LSTM LAYER  (shape) IF THE PREVIOS PAREMETER IS TRUE WE NEED DIFINE THE SHAPE FOR INPUT LAYER\n",
        "    _OUTPUT THE MODEL \n",
        "    \"\"\"\n",
        "    model = Sequential() \n",
        "    if down:\n",
        "        model.add(layers.InputLayer(input_shape=shape))\n",
        "        model.add(Bidirectional(LSTM(units=50, input_shape=(None, 50), return_sequences=True)))\n",
        "        # TextCNN with 4 conv layers\n",
        "        model.add(Conv1D(128, 7, activation='tanh', input_shape=(None, 32)))\n",
        "        model.add(Conv1D(128, 15, activation='tanh'))\n",
        "        model.add(Conv1D(128, 25, activation='tanh'))\n",
        "        model.add(Conv1D(128, 35, activation='tanh'))\n",
        "        model.add(GlobalMaxPooling1D())\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "    else:\n",
        "        model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "        model.add(Bidirectional(LSTM(units=50, input_shape=(None, 50), return_sequences=True))) \n",
        "        model.add(Bidirectional(LSTM(units=50, input_shape=(None, 50), return_sequences=True)))\n",
        "        # TextCNN with 4 conv layers\n",
        "        model.add(Conv1D(128, 7, activation='tanh', input_shape=(None, 32)))\n",
        "        model.add(Conv1D(128, 15, activation='tanh'))\n",
        "        model.add(Conv1D(128, 25, activation='tanh'))\n",
        "        model.add(Conv1D(128, 35, activation='tanh'))\n",
        "        model.add(GlobalMaxPooling1D())\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=myoptimizer,loss=myloss,metrics=[mymetrics])\n",
        "#     model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4UjZIz5s74K"
      },
      "source": [
        "fit_model: this function used to fit our models with 10 epochs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIwv5Sq7tlR5"
      },
      "source": [
        "creat_metrics: this function used to creat dectionary to save result for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jepBWT2q5ylm"
      },
      "outputs": [],
      "source": [
        "def creat_metrics():\n",
        "    model_comparison_table = {}\n",
        "    model_comparison_table['chunk_number'] = []\n",
        "    model_comparison_table['y_test'] = []\n",
        "    model_comparison_table['y_predict'] = []\n",
        "    return model_comparison_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDC-BG9LuDKP"
      },
      "source": [
        "inputer: this function used to Fill dictionary with result evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppsfgc_R5ylm"
      },
      "outputs": [],
      "source": [
        "def inputer(model_comparison_table,chunk_nuber,y_predict,y_test):\n",
        "    y_predict[y_predict>0.5] = 1\n",
        "    model_comparison_table['chunk_number'].append(\"chunk_number \"+str(chunk_nuber))\n",
        "    model_comparison_table['y_test'].append(y_test)\n",
        "    model_comparison_table['y_predict'].append(y_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs8er-dO07K4"
      },
      "source": [
        "chunking_data: this function used to split chunk to train & test & validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLoeG2oB5yln"
      },
      "outputs": [],
      "source": [
        "def chunking_data(Data,test_size=0.20,train_size=0.80,validation_size=0.0):\n",
        "\n",
        "    \"\"\"\n",
        "    _INPUT (Data) CHUNK OF DATASET (validation_size=0.10,test_size=0.20,train_size=0.70) SIZE OF SPLITNG\n",
        "    _OUTPUT (X_train , y_train , X_test , y_test , X_validation , y_validation)\n",
        "    \"\"\"\n",
        "    \n",
        "    test_data = Data.iloc[:int((Data.shape[0]*test_size))]\n",
        "    \n",
        "    # validation_data = Data.iloc[int((Data.shape[0]*test_size)):(int((Data.shape[0]*validation_size))+int((Data.shape[0]*test_size)))]\n",
        "    \n",
        "    train_data = Data.iloc[int((Data.shape[0]*test_size)):]    # +int((Data.shape[0]*validation_size))):]\n",
        "    \n",
        "    X_test = test_data[test_data.columns[-2]]\n",
        "    y_test = test_data[test_data.columns[-1]]\n",
        "    \n",
        "    # X_validation = validation_data[validation_data.columns[-2]]\n",
        "    # y_validation = validation_data[validation_data.columns[-1]]\n",
        "    \n",
        "    X_train = train_data[train_data.columns[-2]]\n",
        "    y_train = train_data[train_data.columns[-1]]\n",
        "    \n",
        "    return X_train , y_train , X_test , y_test #, X_validation , y_validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80_-gjRF1rOt"
      },
      "source": [
        "read__script: this function use to read path script from dataframe then read the js file by this path and finally call sequence_of_syntax_units on this file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF-BlLhI5yln"
      },
      "outputs": [],
      "source": [
        "def read__script(script_paths,parent_path,y):\n",
        "    x_temp = []\n",
        "    y_temp = []\n",
        "    todrop = []\n",
        "    for path in script_paths:\n",
        "        try:\n",
        "            with open(os.path.join(parent_path,path), 'r') as js_file: # , encoding='utf-8'\n",
        "                script = js_file.read()\n",
        "            js_file.close()\n",
        "#             script = lowering_remove_url_ip_(script)\n",
        "            script = sequence_of_syntax_units(script)\n",
        "            new_y = y[script_paths.index(path)]\n",
        "            x_temp.append(script)\n",
        "            y_temp.append(new_y)\n",
        "        except:\n",
        "            pass\n",
        "    if len(x_temp)>0:\n",
        "        return x_temp , np.array(y_temp)\n",
        "    else:\n",
        "        return None , None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwaWPCDd5yln",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "ece68027-b52d-415c-abf7-f8b5caacc8c8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7f8783e340b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_result_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreat_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbottom_model_result_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreat_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_is_created\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_tow_is_created\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# i = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'creat_metrics' is not defined"
          ]
        }
      ],
      "source": [
        "model_result_table = creat_metrics()\n",
        "bottom_model_result_table = creat_metrics()\n",
        "model_is_created = False\n",
        "model_tow_is_created = False\n",
        "\n",
        "filepath_1 =\"/content/drive/MyDrive/Colab Notebooks/Muawiya/results/models/model/weights.best.hdf5\"\n",
        "checkpoint_1 = ModelCheckpoint(filepath_1, monitor='val_accuracy', verbose=False, save_best_only=True, mode='max')\n",
        "es_1 = EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "callbacks_list_1 = [GarbageCollectorCallback(),checkpoint_1, es_1]\n",
        "\n",
        "filepath_2 =\"/content/drive/MyDrive/Colab Notebooks/Muawiya/results/models/bottom_model/weights.best.hdf5\"\n",
        "checkpoint_2 = ModelCheckpoint(filepath__2, monitor='val_accuracy', verbose=False, save_best_only=True, mode='max')\n",
        "es_2 = EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "callbacks_list_2 = [GarbageCollectorCallback(),checkpoint_2, es_2]\n",
        "\n",
        "\n",
        "for i,df_iterator in enumerate(pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Muawiya/XSS_dataset_esprima.csv',chunksize=2000)):\n",
        "    print(\"===============================\"+str(i)+\"=======================================\")\n",
        "    X_train , y_train , X_test , y_test  = chunking_data(df_iterator)\n",
        "    del df_iterator\n",
        "    # X_train , y_train = read__script(X_train.tolist(),\"/content/drive/MyDrive/Colab Notebooks\",y_train.tolist())\n",
        "    # X_test , y_test = read__script(X_test.tolist(),\"/content/drive/MyDrive/Colab Notebooks/\",y_test.tolist())\n",
        "    # X_validation , y_validation = read__script(X_validation.tolist(),\"/content/drive/MyDrive/Colab Notebooks/\",y_validation.tolist())\n",
        "    X_train = preprocessing_dataset_(X_train,fit = True)\n",
        "    X_test = preprocessing_dataset_(X_test,fit=False)\n",
        "    if not model_is_created:\n",
        "      model = create_model()\n",
        "      model.save(\"/content/drive/MyDrive/Colab Notebooks/Muawiya/results/models/model/model chunk \"+str(i)+\".h5\")\n",
        "      model_is_created = True\n",
        "    else:\n",
        "      model = load_model(\"/content/drive/MyDrive/Colab Notebooks/Muawiya/results/models/model/model chunk \"+str(i-1)+\".h5\")\n",
        "    \n",
        "    checkpoint = ModelCheckpoint(filepath_1, monitor='val_accuracy', verbose=False, save_best_only=True, mode='max')\n",
        "    es = EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "    callbacks_list = [GarbageCollectorCallback(),checkpoint, es]\n",
        "    history_Model = model.fit(X_train, y_train, epochs=10,  validation_split=0.20,callbacks=callbacks_list_1)\n",
        "    \n",
        "    top_model = Model(inputs=model.input,outputs=model.layers[1].output)\n",
        "    inputer(model_result_table,i,model.predict(X_test),y_test)\n",
        "    model.save(\"/content/drive/MyDrive/Colab Notebooks/Muawiya/results/models/model/model chunk \"+str(i)+\".h5\")\n",
        "    keras_function_train = K.function([model.input], [model.layers[1].output])\n",
        "    X_train = keras_function_train([X_train, 1])[0]\n",
        "    X_test = top_model.predict(X_test)\n",
        "    X_train , y_train = feature_selection(X_train,y_train)\n",
        "    if not model_tow_is_created:\n",
        "        bottom_model = create_model(shape=X_train[0].shape,down=True)\n",
        "        bottom_model.save(\"/content/drive/MyDrive/Colab Notebooks/Muawiya/results/models/bottom_model/bottom_model chunk \"+str(i)+\".h5\")\n",
        "        model_tow_is_created = True \n",
        "    else:\n",
        "        bottom_model = load_model(\"/content/drive/MyDrive/Colab Notebooks/Muawiya/results/models/bottom_model/bottom_model chunk \"+str(i-1)+\".h5\")\n",
        "    history_Model_bottom_model = bottom_model.fit(X_train, y_train, epochs=10,  validation_split=0.20,callbacks=callbacks_list_2)\n",
        "    inputer(model_result_table,i,bottom_model.predict(X_test),y_test)\n",
        "    bottom_model.save(\"/content/drive/MyDrive/Colab Notebooks/Muawiya/results/models/bottom_model/bottom_model chunk \"+str(i)+\".h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQF5S1Mv5ylo"
      },
      "outputs": [],
      "source": [
        "save_object(model_result_table, \"model_result_table\")\n",
        "save_object(bottom_model_result_table, \"bottom_model_result_table\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save_object(history_Model,\"history_Model\")\n",
        "#save_object(history_Model_bottom_model,'history_Model_bottom_model')"
      ],
      "metadata": {
        "id": "YHuykAq2oFFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMLebo1a5ylp"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(model_result_table)\n",
        "df_ = pd.DataFrame(bottom_model_result_table)\n",
        "df_.to_csv('/content/drive/MyDrive/Colab Notebooks/Muawiya/results/models/bottom_model_result_table.csv',index=False)\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/Muawiya/results/models/model_result_table.csv',index=False)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdVanTPMA2P_"
      },
      "outputs": [],
      "source": [
        "df_.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PygqwIBw2cjV"
      },
      "outputs": [],
      "source": [
        "plot_history(history_Model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history_Model_bottom_model)"
      ],
      "metadata": {
        "id": "n2J0TCISgxzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_result_table['']\n",
        "# bottom_model_result_table[]\n",
        "# evaluation(model,y_predict,y_test)"
      ],
      "metadata": {
        "id": "mgwPG3AKrsRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
        "#     \"\"\"pretty print for confusion matrixes\"\"\"\n",
        "#     columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
        "#     empty_cell = \" \" * columnwidth\n",
        "#     # Print header\n",
        "#     print(\"    \" + empty_cell, end=\" \")\n",
        "#     for label in labels:\n",
        "#         print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
        "#     print()\n",
        "#     # Print rows\n",
        "#     for i, label1 in enumerate(labels):\n",
        "#         print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
        "#         for j in range(len(labels)):\n",
        "#             cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
        "#             if hide_zeroes:\n",
        "#                 cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
        "#             if hide_diagonal:\n",
        "#                 cell = cell if i != j else empty_cell\n",
        "#             if hide_threshold:\n",
        "#                 cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
        "#             print(cell, end=\" \")\n",
        "#         print()"
      ],
      "metadata": {
        "id": "rLXZIDFcrsT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# y_test , y_predicted = np.array(y_test) , np.array(y_predicted)\n",
        "\n",
        "\n",
        "# confusion = confusion_matrix(y_test, y_predicted) # y_test, y_pridicte is numpy array\n",
        "\n",
        "# print_cm(confusion, ['Zero', 'One'])"
      ],
      "metadata": {
        "id": "upgjYZ5SrsWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# print('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_predicted))) # y_test, y_pridicte is numpy array\n",
        "# print('Precision: {:.2f}'.format(precision_score(y_test, y_predicted))) # y_test, y_pridicte is numpy array\n",
        "# print('Recall: {:.2f}'.format(recall_score(y_test, y_predicted))) # y_test, y_pridicte is numpy array\n",
        "# print('F1: {:.2f}'.format(f1_score(y_test, y_predicted))) # y_test, y_pridicte is numpy array"
      ],
      "metadata": {
        "id": "UQq5vlT6rsYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Combined report with all above metrics\n",
        "# from sklearn.metrics import classification_report\n",
        "\n",
        "# print(classification_report(y_test, tree_predicted, target_names=[' 0 ', ' 1 ']))"
      ],
      "metadata": {
        "id": "WYNrz32Wrsat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V0m2U6UGu6Js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1BIeXkUfu6MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s5S4TxVIrsdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0zQS3qBLL8Lw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}