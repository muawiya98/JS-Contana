{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.backend import clear_session\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Activation, Dense, Reshape \n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, Conv1D, GlobalMaxPooling1D\n",
    "from keras.models import load_model\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import esprima as esp\n",
    "import re\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import Model\n",
    "from keras.utils import Sequence\n",
    "import pydot\n",
    "import gensim \n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "from keras import backend as K\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Set the random seed for reproducible results\n",
    "np.random.seed(123)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename+\".pkl\", 'wb') as outp:\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    outp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_object(filename):\n",
    "    with open(filename+\".pkl\", 'rb') as outp:\n",
    "        loaded_object = pickle.load(outp)\n",
    "    outp.close()\n",
    "    return loaded_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_dataset_(scripts,fit = True):\n",
    "    if fit:\n",
    "        tokenizer = Tokenizer(num_words=num_words)\n",
    "        tokenizer.fit_on_texts(scripts)\n",
    "        our_tokenizer = save_object(tokenizer, \"our_tokenizer\")\n",
    "    else :\n",
    "        tokenizer = load_object(\"our_tokenizer\")\n",
    "    X = tokenizer.texts_to_sequences(scripts)\n",
    "    if fit:\n",
    "        vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "        save_object(vocab_size, \"vocab_size\")\n",
    "    X = pad_sequences(X, padding='post', maxlen=maxlen)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert java script code to the abstract syntax tree\n",
    "# then extract the sequences of syntax units with detailed information\n",
    "def sequence_of_syntax_units(scripts):\n",
    "    \"\"\"\n",
    "    sctipts : list or single javascript code \n",
    "    return sequence of syntax units\n",
    "    each syntax unit correspond to line in an abstract syntax tree\n",
    "    \"\"\"\n",
    "    if type(scripts) is list:\n",
    "        # esp.parseScript(script) returns abstract syntax tree of each js scripts\n",
    "        return [re.sub('\\s+', ' ', ''.join(str(esp.parseScript(script)).split('\\n'))) for script in scripts]\n",
    "    elif type(scripts) is str:\n",
    "        return re.sub('\\s+', ' ', ''.join(str(esp.parseScript(scripts)).split('\\n')))\n",
    "    else:\n",
    "        raise ValueError('The type of scripts parameter must be {list or string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word tokens\n",
    "def sequence_of_syntax_units_to_tokens(seq_of_syn_units):\n",
    "    \"\"\"\n",
    "    Convert Sequence of syntax units to tokens to train  word2vec model \n",
    "    -----------------------------------------\n",
    "    seq_of_syn_units: list of sequence syntax units correspond to abstract syntax trees\n",
    "    \"\"\"\n",
    "    return list(map(word_tokenize, seq_of_syn_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGen(Sequence):\n",
    "    \n",
    "    def __init__(self, df, X_col, y_col,batch_size,fit=False,shuffle=True,parent_path=\"\"):\n",
    "        \n",
    "        self.df = df.copy()\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.batch_size = batch_size\n",
    "        self.fit = fit\n",
    "        self.shuffle = shuffle\n",
    "        self.parent_path = parent_path\n",
    "        self.n = df.shape[0]\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    def __get_input(self, path):\n",
    "        try:\n",
    "            with open(os.path.join(self.parent_path,path), 'r', encoding='utf-8') as js_file:\n",
    "                script = js_file.read()\n",
    "            script = sequence_of_syntax_units(script)\n",
    "            return script\n",
    "        except:\n",
    "            return 'None'\n",
    "    def __get_output(self, label):\n",
    "        return label\n",
    "#         return keras.utils.to_categorical(label, num_classes=2)\n",
    "    \n",
    "    def __get_data(self, batches):\n",
    "\n",
    "        path_batch = batches[self.X_col['path']]\n",
    "        \n",
    "        name_batch = batches[self.y_col['name']]\n",
    "\n",
    "        X_batch = np.asarray([self.__get_input(x) for x in path_batch]).astype('object')\n",
    "        \n",
    "        y_batch = np.asarray([self.__get_output(y) for y in name_batch]).astype('object')\n",
    "        \n",
    "        index = np.where(X_batch=='None')[0]\n",
    "        \n",
    "        np.delete(X_batch, index, axis=0)\n",
    "        np.delete(y_batch, index, axis=0)\n",
    "        \n",
    "        X_batch  = preprocessing_dataset_(X_batch , fit = self.fit )\n",
    "        \n",
    "        X_batch = K.cast_to_floatx(X_batch)\n",
    "        y_batch = K.cast_to_floatx(y_batch)\n",
    "        \n",
    "\n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        batches = self.df[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X, y = self.__get_data(batches)        \n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(x_chunk,y_chunk):\n",
    "    indexes = random.sample(range(3, x_chunk.shape[0]), random.randint(1, x_chunk.shape[0]-3))\n",
    "    x_chunk = x_chunk[indexes]\n",
    "    y_chunk = y_chunk[indexes]\n",
    "    return x_chunk,y_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model,history,X_train,y_train,X_test,y_test,model_comparison_table):\n",
    "    train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "    predicted = model.predict(X_test)\n",
    "    Classification_report = classification_report(np.round(model.predict(X_test)), y_test)\n",
    "    inputer(model_comparison_table,i,train_loss, train_accuracy,test_loss, test_accuracy,str(Classification_report))#,precision,recall=0,f1=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(shape=100,without_lstm=False):\n",
    "    model = Sequential()\n",
    "    if not without_lstm:\n",
    "        model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "        model.add(Bidirectional(LSTM(units=50, input_shape=(None, 50), return_sequences=True)))\n",
    "    else :\n",
    "         model.add(layers.InputLayer(input_shape=shape))\n",
    "        \n",
    "    \n",
    "    model.add(Bidirectional(LSTM(units=50, input_shape=(None, 50), return_sequences=True)))\n",
    "\n",
    "    # TextCNN with 4 conv layers\n",
    "    model.add(Conv1D(128, 7, activation='tanh', input_shape=(None, 32)))\n",
    "    model.add(Conv1D(128, 15, activation='tanh'))\n",
    "    model.add(Conv1D(128, 25, activation='tanh'))\n",
    "    model.add(Conv1D(128, 35, activation='tanh'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=myoptimizer,\n",
    "                  loss=myloss,\n",
    "                  metrics=[mymetrics])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model,X_train,y_train,X_validation,y_validation):\n",
    "    history = model.fit(X_train, y_train,epochs=10,verbose=False,validation_data=(X_validation, y_validation))\n",
    "    return model , history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 100)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('G:/JSContanaDataSet/script_path_data.csv')\n",
    "test_df = pd.read_csv('G:/JSContanaDataSet/script_path_test_data.csv')\n",
    "\n",
    "train_batch_size = 20\n",
    "test_batch_size = int(test_df[:500].shape[0]/(train_df[:1000].shape[0]/train_batch_size))\n",
    "validate_batch_size = int(train_df[1000:1200].shape[0]/(train_df[:1000].shape[0]/train_batch_size))\n",
    "# num_words = 400000\n",
    "num_words = 400\n",
    "# maxlen = 1024\n",
    "maxlen = 100\n",
    "embedding_dim = 50\n",
    "myoptimizer = 'adam'\n",
    "myloss= 'binary_crossentropy'\n",
    "mymetrics = 'accuracy'\n",
    "\n",
    "traingen = CustomDataGen(train_df[:1000],\n",
    "                         X_col={'path':'ScriptPath'},\n",
    "                         y_col={'name': 'Label'},\n",
    "                         batch_size=train_batch_size,\n",
    "                         parent_path='G:/JSContanaDataSet/',\n",
    "                         fit=True)\n",
    "\n",
    "validategen = CustomDataGen(train_df[1000:1200],\n",
    "                         X_col={'path':'ScriptPath'},\n",
    "                         y_col={'name': 'Label'},\n",
    "                         batch_size=validate_batch_size,\n",
    "                         parent_path='G:/JSContanaDataSet/',\n",
    "                         fit=False)\n",
    "\n",
    "testgen = CustomDataGen(test_df[:500],\n",
    "                         X_col={'path':'ScriptPath'},\n",
    "                         y_col={'name': 'Label'},\n",
    "                         batch_size=test_batch_size,\n",
    "                         parent_path='G:/JSContanaDataSet/',\n",
    "                         fit=False)\n",
    "\n",
    "X_train , y_train = traingen.__getitem__(0)\n",
    "\n",
    "X_train_chunks = [] \n",
    "y_train_chunks = []\n",
    "\n",
    "X_train_chunks.append(X_train)\n",
    "y_train_chunks.append(y_train)\n",
    "\n",
    "for i in range(1,int(train_df[:1000].shape[0]/train_batch_size)):\n",
    "    X_train_chunks.append(traingen.__getitem__(i)[0])\n",
    "    y_train_chunks.append(traingen.__getitem__(i)[1])\n",
    "\n",
    "X_test , y_test = testgen.__getitem__(0)\n",
    "\n",
    "X_test_chunks = [] \n",
    "y_test_chunks = []\n",
    "\n",
    "X_test_chunks.append(X_test)\n",
    "y_test_chunks.append(y_test)\n",
    "\n",
    "for i in range(1,int(test_df[:500].shape[0]/test_batch_size)):\n",
    "    X_test_chunks.append(testgen.__getitem__(i)[0])\n",
    "    y_test_chunks.append(testgen.__getitem__(i)[1])\n",
    "\n",
    "\n",
    "X_validate , y_validate = validategen.__getitem__(0)\n",
    "\n",
    "X_validate_chunks = [] \n",
    "y_validate_chunks = []\n",
    "\n",
    "X_validate_chunks.append(X_validate)\n",
    "y_validate_chunks.append(y_validate)\n",
    "\n",
    "for i in range(1,int(train_df[1000:1200].shape[0]/validate_batch_size)):\n",
    "    X_validate_chunks.append(validategen.__getitem__(i)[0])\n",
    "    y_validate_chunks.append(validategen.__getitem__(i)[1])\n",
    "\n",
    "vocab_size = load_object('vocab_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_table = {}\n",
    "model_comparison_table['chunk_nuber'] = []\n",
    "model_comparison_table['train_loss'] = []\n",
    "model_comparison_table['train_accuracy'] = []\n",
    "model_comparison_table['test_loss'] = []\n",
    "model_comparison_table['test_accuracy'] = []\n",
    "model_comparison_table['classification_report'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputer(model_comparison_table,chunk_nuber,train_loss, train_accuracy,test_loss,test_accuracy,classification_report):#precision_score,recall_score,f1_score):\n",
    "    model_comparison_table['chunk_nuber'].append(str(chunk_nuber))\n",
    "    model_comparison_table['train_loss'].append(train_loss)\n",
    "    model_comparison_table['train_accuracy'].append(train_accuracy)\n",
    "    model_comparison_table['test_loss'].append(test_loss)    \n",
    "    model_comparison_table['test_accuracy'].append(test_accuracy)  \n",
    "    model_comparison_table['classification_report'].append(classification_report)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ 0 ================\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000017CDA433EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "================ 1 ================\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000017C9561D5E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "================ 2 ================\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000017CF3634E58> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "================ 3 ================\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000017CC8575318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "================ 4 ================\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000017CF210D678> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "================ 5 ================\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000017D08BF9438> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "================ 6 ================\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000017CA9A3D8B8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "================ 7 ================\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000017CAE2E10D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "================ 8 ================\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000017CA0FB5EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "================ 9 ================\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000017CF37AC828> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "train_test_Model = create_model()\n",
    "train_test_Model.save(\"train_test_Model.h5\")\n",
    "for i in range((len(X_train_chunks)/2)):\n",
    "    print(\"================ \"+str(i)+\" ================\")\n",
    "    train_test_Model = load_model(\"train_test_Model.h5\")\n",
    "    X_train,y_train = X_train_chunks[i],y_train_chunks[i]\n",
    "    X_validation,y_validation = X_test_chunks[i],y_test_chunks[i]\n",
    "    train_test_Model , history_train_test_Model = fit_model(train_test_Model,X_train,y_train,X_validation,y_validation)\n",
    "    evaluation(train_test_Model,history_train_test_Model,X_train,y_train,X_validation,y_validation,model_comparison_table)\n",
    "    train_test_Model.save(\"train_test_Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_nuber</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>classification_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.204410</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.745450</td>\n",
       "      <td>0.6</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.009919</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.626090</td>\n",
       "      <td>0.3</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.606332</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.784802</td>\n",
       "      <td>0.5</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.591738</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.653741</td>\n",
       "      <td>0.6</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.656104</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.718224</td>\n",
       "      <td>0.5</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chunk_nuber  train_loss  train_accuracy  test_loss  test_accuracy  \\\n",
       "0           0    0.204410            0.90   0.745450            0.6   \n",
       "1           1    0.009919            1.00   2.626090            0.3   \n",
       "2           2    0.606332            0.65   0.784802            0.5   \n",
       "3           3    0.591738            0.75   0.653741            0.6   \n",
       "4           4    0.656104            0.50   0.718224            0.5   \n",
       "\n",
       "                               classification_report  \n",
       "0                precision    recall  f1-score   ...  \n",
       "1                precision    recall  f1-score   ...  \n",
       "2                precision    recall  f1-score   ...  \n",
       "3                precision    recall  f1-score   ...  \n",
       "4                precision    recall  f1-score   ...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(model_comparison_table)\n",
    "df.to_csv('G:/JSContanaDataSet/result_df.csv',index=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n         0.0       0.50      0.50      0.50         4\\n         1.0       0.67      0.67      0.67         6\\n\\n    accuracy                           0.60        10\\n   macro avg       0.58      0.58      0.58        10\\nweighted avg       0.60      0.60      0.60        10\\n'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"classification_report\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.50      0.50         4\n",
      "         1.0       0.67      0.67      0.67         6\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.58      0.58      0.58        10\n",
      "weighted avg       0.60      0.60      0.60        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df[\"classification_report\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_function = K.function([train_test_Model.input], [train_test_Model.layers[1].output])\n",
    "for_traingen = []\n",
    "for i in range(len(X_train_chunks)):\n",
    "    first_layer_output = keras_function([X_train_chunks[i], 1])\n",
    "    for_traingen.append(first_layer_output[0])\n",
    "save_object(for_traingen, \"for_traingen\")\n",
    "for_traingen = load_object(\"for_traingen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_table_1 = {}\n",
    "model_comparison_table_1['chunk_nuber'] = []\n",
    "model_comparison_table_1['train_loss'] = []\n",
    "model_comparison_table_1['train_accuracy'] = []\n",
    "model_comparison_table_1['test_loss'] = []\n",
    "model_comparison_table_1['test_accuracy'] = []\n",
    "model_comparison_table_1['classification_report'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_test_Model = create_model()\n",
    "validate_test_Model.save(\"validate_test_Model.h5\")\n",
    "for i in range(len(X_validate_chunks)):\n",
    "    print(\"================ \"+str(i)+\" ================\")\n",
    "    validate_test_Model = load_model(\"validate_test_Model.h5\")\n",
    "    X_train,y_train = X_validate_chunks[i],y_validate_chunks[i]\n",
    "    X_validation,y_validation = X_test_chunks[i],y_test_chunks[i]\n",
    "    validate_test_Model , history_validate_test_Model = fit_model(validate_test_Model,X_train,y_train,X_validation,y_validation)\n",
    "    evaluation(validate_test_Model,history_validate_test_Model,X_train,y_train,X_validation,y_validation,model_comparison_table_1)\n",
    "    validate_test_Model.save(\"validate_test_Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame(model_comparison_table_1)\n",
    "df_1.to_csv('G:/JSContanaDataSet/result_df_1.csv',index=False)\n",
    "df_1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_function = K.function([validate_test_Model.input], [validate_test_Model.layers[1].output])\n",
    "for_validate = []\n",
    "for i in range(len(X_validate_chunks)):\n",
    "    first_layer_output = keras_function([X_validate_chunks[i], 1])\n",
    "    for_validate.append(first_layer_output[0])\n",
    "save_object(for_validate, \"for_validate\")\n",
    "for_validate = load_object(\"for_validate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_table_2 = {}\n",
    "model_comparison_table_2['chunk_nuber'] = []\n",
    "model_comparison_table_2['train_loss'] = []\n",
    "model_comparison_table_2['train_accuracy'] = []\n",
    "model_comparison_table_2['test_loss'] = []\n",
    "model_comparison_table_2['test_accuracy'] = []\n",
    "model_comparison_table_2['classification_report'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_Model = create_model(shape=for_traingen[0][0].shape,without_lstm=True)\n",
    "lstm_Model.save(\"lstm_Model.h5\")\n",
    "for i in range(len(for_traingen)):\n",
    "    print(\"================ \"+str(i)+\" ================\")\n",
    "    lstm_Model = load_model(\"lstm_Model.h5\")\n",
    "    X_train,y_train = feature_selection(for_traingen[i],y_train_chunks[i])\n",
    "    X_validation,y_validation = feature_selection(for_validate[i],X_validate_chunks[i])\n",
    "    lstm_Model , history_lstm_Model = fit_model(lstm_Model,X_train,y_train,X_validation,y_validation)\n",
    "    evaluation(lstm_Model,history_lstm_Model,X_train,y_train,X_validation,y_validation,model_comparison_table_2)\n",
    "    lstm_Model.save(\"lstm_Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.DataFrame(model_comparison_table_2)\n",
    "df_2.to_csv('G:/JSContanaDataSet/result_df_2.csv',index=False)\n",
    "df_2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
