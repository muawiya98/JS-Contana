{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V5WsBnKyPFMa",
    "outputId": "f29cc846-9b1e-4523-dbbb-262e9dd0edb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: tensorflow==2.3.1 in /usr/local/lib/python3.7/dist-packages (2.3.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.14.1)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.37.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (3.17.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.2.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (2.10.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.48.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.3.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.35.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.8.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (57.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.9)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2022.6.15)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.2.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: q in /usr/local/lib/python3.7/dist-packages (2.7)\n",
      "Requirement already satisfied: keras==2.4.3 in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (1.18.5)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (6.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.4.3) (1.15.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: esprima in /usr/local/lib/python3.7/dist-packages (4.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.3.1\n",
    "!pip install q keras==2.4.3\n",
    "!pip install esprima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zXCUlR9rx4o6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.backend import clear_session\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Activation, Dense, Reshape \n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, Conv1D, GlobalMaxPooling1D\n",
    "from keras.models import load_model\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "from sklearn.metrics import classification_report , confusion_matrix\n",
    "import esprima as esp\n",
    "import re\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import Model\n",
    "from keras.utils import Sequence\n",
    "import pydot\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "from keras import backend as K\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import logging\n",
    "from time import time\n",
    "from functools import partial\n",
    "import concurrent.futures\n",
    "from threading import current_thread\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from keras.layers import Input\n",
    "from keras.models import Model , clone_model\n",
    "np.random.seed(123)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NeuNp9L-7i4G",
    "outputId": "53bbf010-4a6a-4314-d528-93ff605cbc9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkgxeec8fEjT"
   },
   "source": [
    "Hyperparameter for Our Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "F5OTb7eq5ylf"
   },
   "outputs": [],
   "source": [
    "vocab_size = 100000\n",
    "num_words = 400000\n",
    "maxlen = 1024\n",
    "embedding_dim = 50\n",
    "myoptimizer = 'adam'\n",
    "myloss= 'binary_crossentropy'\n",
    "mymetrics = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0csThO54fsJV"
   },
   "source": [
    "save_object & load_object to save and load All objects we are need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MuB9VYRXPFMf"
   },
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    \"\"\"\n",
    "    _ INPUT (obj) THE OBJECT WE NEED SAVW IT (filename) THE NAME OF OBJECT\n",
    "    \"\"\"\n",
    "    filename = os.path.join('/content/drive/MyDrive/Colab Notebooks/results/par',filename)\n",
    "    with open(filename+\".pkl\", 'wb') as outp:\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    outp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "w32njEwEPFMf"
   },
   "outputs": [],
   "source": [
    "def load_object(filename):\n",
    "    \"\"\"\n",
    "    _ INPUT THE NAME OF OBJECT WE NEED LOAD IT\n",
    "    \"\"\"\n",
    "    filename = os.path.join('/content/drive/MyDrive/Colab Notebooks/results/par',filename)\n",
    "    with open(filename+\".pkl\", 'rb') as outp:\n",
    "        loaded_object = pickle.load(outp)\n",
    "    outp.close()\n",
    "    return loaded_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uxEPSqgNPFMg"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jp3MOabsgYmF"
   },
   "source": [
    "lowering_remove_url_ip_: this function used to cleaning js scripts \n",
    "\n",
    "\n",
    "1.   remove urls\n",
    "2.   remove ips\n",
    "3.   remove comments\n",
    "4.   convert to lower case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9ko40zJGqnCW"
   },
   "outputs": [],
   "source": [
    "def lowering_remove_url_ip_(script):\n",
    "    \"\"\"\n",
    "    _ INPUT IS JS FILE \n",
    "    _ OUTPUT IS CLEAN JS FILE\n",
    "    \"\"\"\n",
    "    script = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', script)\n",
    "    script = re.sub(r'((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)', '', script)\n",
    "    script = re.sub(r\"(/\\*([^*]|[\\r\\n]|(\\*+([^*/]|[\\r\\n])))*\\*+/)|(//.*)\",'',script)\n",
    "    script = script.lower()\n",
    "    return script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdvPs5n9kooY"
   },
   "source": [
    "preprocessing_dataset_: this function used to tokenzation js file and convert text to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0XqcEGYKPFMg"
   },
   "outputs": [],
   "source": [
    "def preprocessing_dataset_(scripts,fit = True):\n",
    "    \"\"\"\n",
    "    _INPUT (scripts) JS FILES (fit) IF THE FILES IS FROM TRANING SET IT WILL BE (fit=True) ELSE IT WILL BE (fit=False)\n",
    "    \"\"\"\n",
    "    if fit:\n",
    "        tokenizer = Tokenizer(num_words=num_words)\n",
    "        tokenizer.fit_on_texts(scripts)\n",
    "        our_tokenizer = save_object(tokenizer, \"our_tokenizer\")\n",
    "    else :\n",
    "        tokenizer = load_object(\"our_tokenizer\")\n",
    "    X = tokenizer.texts_to_sequences(scripts)\n",
    "    X = pad_sequences(X, padding='post', maxlen=maxlen)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbQbuLePmnOH"
   },
   "source": [
    "sequence_of_syntax_units: this function used to convert java script code to the abstract syntax tree then extract the sequences of syntax units with detailed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "v9BoJm8ZPFMh"
   },
   "outputs": [],
   "source": [
    "def sequence_of_syntax_units(scripts):\n",
    "    \"\"\"\n",
    "    sctipts : list or single javascript code \n",
    "    return sequence of syntax units\n",
    "    each syntax unit correspond to line in an abstract syntax tree\n",
    "    \"\"\"\n",
    "    if type(scripts) is list:\n",
    "        # esp.parseScript(script) returns abstract syntax tree of each js scripts\n",
    "        return [re.sub('\\s+', ' ', ''.join(str(esp.parseScript(script)).split('\\n'))) for script in scripts]\n",
    "    elif type(scripts) is str:\n",
    "        return re.sub('\\s+', ' ', ''.join(str(esp.parseScript(scripts)).split('\\n')))\n",
    "    else:\n",
    "        raise ValueError('The type of scripts parameter must be {list or string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5wZYnCfDPFMi"
   },
   "outputs": [],
   "source": [
    "# create word tokens\n",
    "def sequence_of_syntax_units_to_tokens(seq_of_syn_units):\n",
    "    \"\"\"\n",
    "    Convert Sequence of syntax units to tokens to train  word2vec model \n",
    "    -----------------------------------------\n",
    "    seq_of_syn_units: list of sequence syntax units correspond to abstract syntax trees\n",
    "    \"\"\"\n",
    "    return list(map(word_tokenize, seq_of_syn_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXUt70UAm-Wx"
   },
   "source": [
    "feature_selection: this function used to make initial feature selection on chunk of js file after used the preprocessing_dataset_ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eVR0u9kXPFMj"
   },
   "outputs": [],
   "source": [
    "def feature_selection(x_chunk,y_chunk):\n",
    "    \"\"\"\n",
    "    _INPUT (x_chunk) SET OF JS CODE AFTER APPLY PREPROCESSING DATASET FUNCTION (y_chunk)\n",
    "    \"\"\"\n",
    "    indexes = random.sample(range(3, x_chunk.shape[0]), random.randint(1, x_chunk.shape[0]-3))\n",
    "    x_chunk[indexes,:,:] = 0\n",
    "    return x_chunk,y_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWBgODVloXX4"
   },
   "source": [
    "evaluation: this function used to evaluate our model and save result in comparison table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MZN229zLPFMj"
   },
   "outputs": [],
   "source": [
    "def evaluation(model,chunk_number,history,X_train,y_train,X_test,y_test,model_comparison_table):\n",
    "    \"\"\"\n",
    "    _INPUT \n",
    "    (model) THE MODEL THAT WE NEED EVALUATE\n",
    "    (chunk_number) NUMBER OF CHUNK FROM DATA WE PROCESS\n",
    "    (history) IF WE NEED CALL FUNCTION plot_history\n",
    "    (X_train,y_train,X_test,y_test) TRAINING DATA SET AND TESTING DATA SET\n",
    "    (model_comparison_table) DECTIONARY TO SAVE RESULT [train_loss, train_accuracy,test_loss, test_accuracy,Classification_report]\n",
    "    \"\"\"\n",
    "    train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "    predicted = model.predict(X_test)\n",
    "    Classification_report = classification_report(np.round(model.predict(X_test)), y_test)\n",
    "    inputer(model_comparison_table,chunk_nuber,train_loss, train_accuracy,test_loss, test_accuracy,str(Classification_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2mXR50Sqn_2"
   },
   "source": [
    "create_model: this function used to creat neural network with 11 layer or with 9 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "V9BXBE5JPFMk"
   },
   "outputs": [],
   "source": [
    "def create_model(shape=None,down=False):\n",
    "    \"\"\"\n",
    "    _INPUT (down) IF WE WANT TO CREAT MODEL WITHOUT FIRST LSTM LAYER  (shape) IF THE PREVIOS PAREMETER IS TRUE WE NEED DIFINE THE SHAPE FOR INPUT LAYER\n",
    "    _OUTPUT THE MODEL \n",
    "    \"\"\"\n",
    "    model = Sequential() \n",
    "    if down:\n",
    "        model.add(layers.InputLayer(input_shape=shape))\n",
    "        model.add(Bidirectional(LSTM(units=50, input_shape=(None, 50), return_sequences=True)))\n",
    "        # TextCNN with 4 conv layers\n",
    "        model.add(Conv1D(128, 7, activation='tanh', input_shape=(None, 32)))\n",
    "        model.add(Conv1D(128, 15, activation='tanh'))\n",
    "        model.add(Conv1D(128, 25, activation='tanh'))\n",
    "        model.add(Conv1D(128, 35, activation='tanh'))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "        model.add(Bidirectional(LSTM(units=50, input_shape=(None, 50), return_sequences=True))) \n",
    "        model.add(Bidirectional(LSTM(units=50, input_shape=(None, 50), return_sequences=True)))\n",
    "        # TextCNN with 4 conv layers\n",
    "        model.add(Conv1D(128, 7, activation='tanh', input_shape=(None, 32)))\n",
    "        model.add(Conv1D(128, 15, activation='tanh'))\n",
    "        model.add(Conv1D(128, 25, activation='tanh'))\n",
    "        model.add(Conv1D(128, 35, activation='tanh'))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=myoptimizer,loss=myloss,metrics=[mymetrics])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4UjZIz5s74K"
   },
   "source": [
    "fit_model: this function used to fit our models with 10 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9xKb1B6gPFMk"
   },
   "outputs": [],
   "source": [
    "def fit_model(model,X_train,y_train,X_validation,y_validation):\n",
    "    \"\"\"\n",
    "    _INPUT (model) (X_train,y_train,X_validation,y_validation)\n",
    "    \"\"\"\n",
    "    history = model.fit(X_train, y_train,epochs=10,verbose=False,validation_data=(X_validation, y_validation))\n",
    "    return model , history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIwv5Sq7tlR5"
   },
   "source": [
    "creat_metrics: this function used to creat dectionary to save result for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jepBWT2q5ylm"
   },
   "outputs": [],
   "source": [
    "def creat_metrics():\n",
    "    model_comparison_table = {}\n",
    "    model_comparison_table['chunk_nuber'] = []\n",
    "    model_comparison_table['train_loss'] = []\n",
    "    model_comparison_table['train_accuracy'] = []\n",
    "    model_comparison_table['test_loss'] = []\n",
    "    model_comparison_table['test_accuracy'] = []\n",
    "    model_comparison_table['classification_report'] = []\n",
    "    return model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDC-BG9LuDKP"
   },
   "source": [
    "inputer: this function used to Fill dictionary with result evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ppsfgc_R5ylm"
   },
   "outputs": [],
   "source": [
    "def inputer(model_comparison_table,chunk_nuber,train_loss, train_accuracy,test_loss,test_accuracy,classification_report):#precision_score,recall_score,f1_score):\n",
    "    model_comparison_table['chunk_nuber'].append(str(chunk_nuber))\n",
    "    model_comparison_table['train_loss'].append(train_loss)\n",
    "    model_comparison_table['train_accuracy'].append(train_accuracy)\n",
    "    model_comparison_table['test_loss'].append(test_loss)    \n",
    "    model_comparison_table['test_accuracy'].append(test_accuracy)  \n",
    "    model_comparison_table['classification_report'].append(classification_report)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gs8er-dO07K4"
   },
   "source": [
    "chunking_data: this function used to split chunk to train & test & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "kLoeG2oB5yln"
   },
   "outputs": [],
   "source": [
    "def chunking_data(Data,validation_size=0.10,test_size=0.20,train_size=0.70):\n",
    "    \"\"\"\n",
    "    _INPUT (Data) CHUNK OF DATASET (validation_size=0.10,test_size=0.20,train_size=0.70) SIZE OF SPLITNG\n",
    "    _OUTPUT (X_train , y_train , X_test , y_test , X_validation , y_validation)\n",
    "    \"\"\"\n",
    "    test_data = Data.iloc[:int((Data.shape[0]*test_size))]\n",
    "    validation_data = Data.iloc[int((Data.shape[0]*test_size)):(int((Data.shape[0]*validation_size))+int((Data.shape[0]*test_size)))]\n",
    "    train_data = Data.iloc[(int((Data.shape[0]*test_size))+int((Data.shape[0]*validation_size))):]\n",
    "    X_test = test_data[test_data.columns[-2]]\n",
    "    y_test = test_data[test_data.columns[-1]]\n",
    "    X_validation = validation_data[validation_data.columns[-2]]\n",
    "    y_validation = validation_data[validation_data.columns[-1]]\n",
    "    X_train = train_data[train_data.columns[-2]]\n",
    "    y_train = train_data[train_data.columns[-1]]\n",
    "    return X_train , y_train , X_test , y_test , X_validation , y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80_-gjRF1rOt"
   },
   "source": [
    "read__script: this function use to read path script from dataframe then read the js file by this path and finally call sequence_of_syntax_units on this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yF-BlLhI5yln"
   },
   "outputs": [],
   "source": [
    "def read__script(script_paths,parent_path,y):\n",
    "    x_temp = []\n",
    "    y_temp = []\n",
    "    todrop = []\n",
    "    for path in script_paths:\n",
    "        try:\n",
    "            with open(os.path.join(parent_path,path), 'r') as js_file: # , encoding='utf-8'\n",
    "                script = js_file.read()\n",
    "            js_file.close()\n",
    "#             script = lowering_remove_url_ip_(script)\n",
    "            script = sequence_of_syntax_units(script)\n",
    "            new_y = y[script_paths.index(path)]\n",
    "            x_temp.append(script)\n",
    "            y_temp.append(new_y)\n",
    "        except:\n",
    "            pass\n",
    "    if len(x_temp)>0:\n",
    "        return x_temp , np.array(y_temp)\n",
    "    else:\n",
    "        return None , None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GwaWPCDd5yln",
    "outputId": "630c8de4-12b0-4fd5-b812-1af8310918a2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 1024)\n",
      "(230, 1024)\n",
      "(110, 1024)\n",
      "(110, 1024)\n",
      "(110,)\n",
      "(110, 1024, 100)\n",
      "(230, 1024, 100)\n",
      "(784, 1024, 100)\n",
      "(784,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [09:17, 557.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(791, 1024)\n",
      "(226, 1024)\n",
      "(110, 1024)\n",
      "(110, 1024)\n",
      "(110,)\n",
      "(110, 1024, 100)\n",
      "(226, 1024, 100)\n",
      "(791, 1024, 100)\n",
      "(791,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [35:11, 717.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "model_result_table = creat_metrics()\n",
    "bottom_model_result_table = creat_metrics()\n",
    "model_is_created = False\n",
    "model_tow_is_created = False\n",
    "for i , df_iterator in enumerate(pd.read_csv('/content/drive/MyDrive/Colab Notebooks/XSS_DataSet.csv',chunksize=2000)):\n",
    "    try:\n",
    "      X_train , y_train , X_test , y_test , X_validation , y_validation = chunking_data(df_iterator)\n",
    "      X_train , y_train = read__script(X_train.tolist(),\"/content/drive/MyDrive/Colab Notebooks\",y_train.tolist())\n",
    "      X_test , y_test = read__script(X_test.tolist(),\"/content/drive/MyDrive/Colab Notebooks/\",y_test.tolist())\n",
    "      X_validation , y_validation = read__script(X_validation.tolist(),\"/content/drive/MyDrive/Colab Notebooks/\",y_validation.tolist())\n",
    "      if X_train!=None:\n",
    "          X_train = preprocessing_dataset_(X_train , fit = True )\n",
    "          X_test = preprocessing_dataset_(X_test , fit = False )\n",
    "          X_validation = preprocessing_dataset_(X_validation , fit = False )\n",
    "          if not model_is_created:\n",
    "              model = create_model()\n",
    "              model.save(\"/content/drive/MyDrive/Colab Notebooks/results/models/model/model chunk \"+str(i)+\".h5\")\n",
    "              model_is_created = True\n",
    "          else:\n",
    "              model = load_model(\"/content/drive/MyDrive/Colab Notebooks/results/models/model/model chunk \"+str(i-1)+\".h5\")\n",
    "          model , history_Model = fit_model(model,X_train,y_train,X_validation,y_validation)\n",
    "          top_model = Model(inputs=model.input,outputs=model.layers[1].output)\n",
    "          evaluation(model,i,history_Model,X_train,y_train,X_test,y_test,model_result_table)\n",
    "          model.save(\"/content/drive/MyDrive/Colab Notebooks/results/models/model/model chunk \"+str(i)+\".h5\")\n",
    "          keras_function_train = K.function([model.input], [model.layers[1].output])\n",
    "          X_train = keras_function_train([X_train, 1])[0]\n",
    "          X_validation = top_model.predict(X_validation)\n",
    "          X_test = top_model.predict(X_test)\n",
    "          X_train , y_train = feature_selection(X_train,y_train)\n",
    "          if not model_tow_is_created:\n",
    "              bottom_model = create_model(shape=X_train[0].shape,down=True)\n",
    "              bottom_model.save(\"/content/drive/MyDrive/Colab Notebooks/results/models/bottom_model/bottom_model chunk \"+str(i)+\".h5\")\n",
    "              model_tow_is_created = True \n",
    "          else:\n",
    "              bottom_model = load_model(\"/content/drive/MyDrive/Colab Notebooks/results/models/bottom_model/bottom_model chunk \"+str(i-1)+\".h5\")\n",
    "          bottom_model , history_Model_bottom_model = fit_model(bottom_model,X_train,y_train,X_validation,y_validation)\n",
    "          evaluation(bottom_model,i,history_Model_bottom_model,X_train,y_train,X_test,y_test,bottom_model_result_table)\n",
    "          bottom_model.save(\"/content/drive/MyDrive/Colab Notebooks/results/models/bottom_model/bottom_model chunk \"+str(i)+\".h5\")\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQF5S1Mv5ylo"
   },
   "outputs": [],
   "source": [
    "save_object(model_result_table, \"model_result_table\")\n",
    "save_object(bottom_model_result_table, \"bottom_model_result_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMLebo1a5ylp"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(model_result_table)\n",
    "df_ = pd.DataFrame(bottom_model_result_table)\n",
    "df_.to_csv('/content/drive/MyDrive/Colab Notebooks/results/models/bottom_model_result_table.csv',index=False)\n",
    "df.to_csv('/content/drive/MyDrive/Colab Notebooks/results/models/model_result_table.csv',index=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdVanTPMA2P_"
   },
   "outputs": [],
   "source": [
    "df_.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Goag8ek35ylp"
   },
   "outputs": [],
   "source": [
    "for i in df['classification_report']:\n",
    "    print(\"================================\")\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrRwxX-j5ylp"
   },
   "outputs": [],
   "source": [
    "for i in df_['classification_report']:\n",
    "    print(\"================================\")\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqvkZ8J1sqYv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
